{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d747364-6d2c-401e-a51f-0389fea44f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (1.68.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: numpy>=2.0.2 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from openai) (2.0.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from openai) (2.10.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: sounddevice>=0.5.1 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from openai) (0.5.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from openai) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from sounddevice>=0.5.1->openai) (1.17.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.5.1->openai) (2.22)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai\n",
    "!pip install tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0aa844fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import html\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import json\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import openai\n",
    "import base64\n",
    "import pandas as pd\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3fbb35d-a2cc-4563-9ddd-64ac3e647899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN DATA:\n",
      "                                                                                                                                                                                                                                text  \\\n",
      "0              NEW YORK (Reuters) - Apple Inc Chief Executive Steve Jobs sought to soothe investor concerns about his health on Monday, saying his weight loss was caused by a hormone imbalance that is relatively simple to treat.   \n",
      "1                                                          Last week, Citigroup Inc's ( C.N ) Chief Executive Vikram Pandit said that he, Chairman Win Bischoff, and senior adviser Robert Rubin would not receive bonuses for 2008.   \n",
      "2                                                             Lehman Brothers LEH.N shares fell sharply on Monday on speculation that the investment bank could be bought for $15 a share, a price well below current market levels.   \n",
      "3  Franz told Reuters that Fiat Chief Executive Sergio Marchionne had said at a meeting on Monday he foresaw closing Opel's Kaiserslautern engine plant in Germany and other Fiat and Opel manufacturing sites in England and Italy.   \n",
      "4                                                                  \"In an industry that has a poor track record for M&A execution, Fiat CEO Sergio Marchionne has his work cut out for him,\" said Morgan Stanley analyst Adam Jonas.   \n",
      "\n",
      "                                                                                                   annotations  \n",
      "0                      [Apple Inc ; Steve Jobs ; founded_by, Apple Inc ; Steve Jobs ; chief_executive_officer]  \n",
      "1                                                                       [Vikram Pandit ; Citigroup ; employer]  \n",
      "2  [Lehman Brothers ; investment bank ; product_or_material_produced, Lehman Brothers ; investment ; industry]  \n",
      "3                                                                        [Sergio Marchionne ; Fiat ; employer]  \n",
      "4                                                                        [Sergio Marchionne ; Fiat ; employer]  \n",
      "\n",
      "DEV DATA:\n",
      "                                                                                                                                                                                                                        text  \\\n",
      "0                                                                                                 Yum China will become a franchise of Yum Brands in Mainland China, the parent of KFC, Pizza Hut and Taco Bell chains said.   \n",
      "1                                                            Warren Buffett's Berkshire Hathaway (Sao Paolo: BERK34F.SA - news ) this month also launched its first cyber policies through its specialty insurance division.   \n",
      "2  In the wake of last year's attack on Sony Pictures Entertainment, parent Sony Corp said its financial condition could suffer if it were attacked again, since current policies \"might not cover all expenses and losses.\"   \n",
      "3                                                 Oct 12 Investment bank Cantor Fitzgerald Europe, part of Cantor Fitzgerald, has hired four executives from financial services firm Shore Capital for its Edinburgh office.   \n",
      "4                                     Collaborating with University of Connecticut doctoral student Devon Goss, Matthew Hughey researched nearly 24,000 English-language newspaper articles across the globe from 2003-2014.   \n",
      "\n",
      "                                                                                                                                                                                                      annotations  \n",
      "0  [Yum China ; Pizza Hut ; brand, Yum China ; Taco Bell ; brand, Yum Brands ; Taco Bell ; subsidiary, Yum Brands ; Taco Bell ; owner_of, Yum Brands ; Pizza Hut ; subsidiary, Yum Brands ; Pizza Hut ; owner_of]  \n",
      "1                                                                                                                                                                     [Berkshire Hathaway ; insurance ; industry]  \n",
      "2                            [Sony Pictures ; Sony ; owned_by, Sony ; Sony Pictures ; owner_of, Sony Pictures ; Sony ; founded_by, Sony Pictures ; Sony ; parent_organization, Sony ; Sony Pictures ; subsidiary]  \n",
      "3                                                                                      [Cantor Fitzgerald ; financial services ; industry, Cantor Fitzgerald ; financial services ; product_or_material_produced]  \n",
      "4                                                                                                                                                         [Matthew Hughey ; University of Connecticut ; employer]  \n",
      "\n",
      "TEST DATA:\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 text  \\\n",
      "0  Wednesday, July 8, 2015 10:30AM IST (5:00AM GMT) Rimini Street Comment on Oracle Litigation Las Vegas, United States Rimini Street, Inc., the leading independent provider of enterprise software support for SAP AG’s (NYSE:SAP) Business Suite and BusinessObjects software and Oracle Corporation’s (NYSE:ORCL) Siebel , PeopleSoft , JD Edwards , E-Business Suite , Oracle Database , Hyperion and Oracle Retail software, today issued a statement on the Oracle litigation.   \n",
      "1                                                                                                                                                                                                                                                                                      The Daily Show with Trevor Noah premieres tonight... and while the show will be based on Comedy Central, Viacom pans to simulcast the debut across all of its networks, including VH1 and MTV.   \n",
      "2                                                                                                                                                                                                                                                                                                                               \"Our results for the quarter show very balanced performance across our business lines,\" said Citigroup chief executive Michael Corbat in a statement.   \n",
      "3                                                                                                                                                                                                                                          Saudi Arabian budget carrier flynas, which made its first profit this year, is in talks with plane manufacturers Airbus and Boeing as it seeks to purchase four new aircraft over the next four years, its chief executive told reporters.   \n",
      "4                                                                                                                                                                                                                                                                                                                                                        First Eagle is currently owned by members of the Arnhold family, private equity firm TA Associates Management and employees.   \n",
      "\n",
      "                                          annotations  \n",
      "0              [PeopleSoft ; JD Edwards ; subsidiary]  \n",
      "1                           [VH1 ; Viacom ; owned_by]  \n",
      "2             [Michael Corbat ; Citigroup ; employer]  \n",
      "3  [Airbus ; aircraft ; product_or_material_produced]  \n",
      "4         [TA Associates ; private equity ; industry]  \n"
     ]
    }
   ],
   "source": [
    "# Data Import\n",
    "\n",
    "# function to extract sentences and annotations\n",
    "def extract_data(text):\n",
    "    data = []\n",
    "    sentences = text.strip().split('\\n')\n",
    "    for sentence in sentences:\n",
    "        parts = sentence.split('|')\n",
    "        if len(parts) < 2:  # Ensure valid format\n",
    "            continue  # Skip invalid lines\n",
    "        text = parts[0].strip()\n",
    "        annotations = [part.strip() for part in parts[1:]]\n",
    "        data.append({'text': text, 'annotations': annotations})\n",
    "    return data\n",
    "\n",
    "# function to read and process a file\n",
    "def process_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        return extract_data(text)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: File {file_path} not found.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Process train.txt\n",
    "train_data = process_file('train.txt')\n",
    "df_train = pd.DataFrame(train_data) if train_data else pd.DataFrame()\n",
    "if not df_train.empty:\n",
    "    print(\"\\nTRAIN DATA:\")\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    print(df_train.head())\n",
    "else:\n",
    "    print(\"No data extracted from train.txt\")\n",
    "\n",
    "# Process dev.txt\n",
    "dev_data = process_file('dev.txt')\n",
    "df_dev = pd.DataFrame(dev_data) if dev_data else pd.DataFrame()\n",
    "if not df_dev.empty:\n",
    "    print(\"\\nDEV DATA:\")\n",
    "    print(df_dev.head())\n",
    "else:\n",
    "    print(\"No data extracted from dev.txt\")\n",
    "\n",
    "# Process test.txt\n",
    "test_data = process_file('test.txt')\n",
    "df_test = pd.DataFrame(test_data) if test_data else pd.DataFrame()\n",
    "if not df_test.empty:\n",
    "    print(\"\\nTEST DATA:\")\n",
    "    print(df_test.head())\n",
    "else:\n",
    "    print(\"No data extracted from test.txt\")\n",
    "\n",
    "# Three separate DataFrames:\n",
    "# df_train - containing training data\n",
    "# df_dev - containing development/validation data\n",
    "# df_test - containing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c478aff-c2ae-4659-be25-b97368c64ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "\n",
    "# Clean special characters\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text by:\n",
    "    1. Decoding HTML entities (like &#039; &#959; etc.)\n",
    "    2. Removing HTML tags (like <p> </p>)\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    # Decode HTML entities (converts &#039; to ', &#959; to ο, etc.)\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "    # Replace Unicode right single quotation mark with a standard single quote\n",
    "    text = text.replace('\\u2019', \"'\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply cleaning to the 'text' column in all three dataframes\n",
    "for df in [df_train, df_dev, df_test]:\n",
    "    if not df.empty and 'text' in df.columns:\n",
    "        df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Clean sources' information & tickers\n",
    "def clean_source_info(text):\n",
    "    \"\"\"\n",
    "    Clean source information from the beginning of text.\n",
    "    Looks for specific patterns in the first 50 characters and removes everything\n",
    "    from the beginning to the end of the matched pattern.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    # Check only the first 50 characters (or all if less than 50)\n",
    "    first_part = text[:50] if len(text) > 50 else text\n",
    "    \n",
    "    # Patterns to look for in the beginning\n",
    "    patterns = [\n",
    "        r'\\(AP\\) -+ ',\n",
    "        r'\\(AP\\) _ ',\n",
    "        r'\\(AP\\) — ',\n",
    "        r'\\(IANS\\) ',\n",
    "        r'\\(Reuters\\) - ',\n",
    "        r'\\(TheStreet\\) -- ',\n",
    "        r'\\(GLOBE NEWSWIRE\\) -- ',\n",
    "        r'\\(ShareCast\\) - \\(ShareCast News\\) - ',\n",
    "        r'CNBC CNBC\\.com SHARES ',\n",
    "        r'BST - '\n",
    "    ]\n",
    "    \n",
    "    # Find the earliest match of any pattern\n",
    "    earliest_match = None\n",
    "    earliest_end = len(text)\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, first_part)\n",
    "        if match and match.end() < earliest_end:\n",
    "            earliest_match = match\n",
    "            earliest_end = match.end()\n",
    "    \n",
    "    # If a match was found, remove everything up to the end of the match\n",
    "    if earliest_match:\n",
    "        return text[earliest_end:].strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_source_info_2(text):\n",
    "    \"\"\"\n",
    "    Cleans a text string based on the presence of \" IST \" and \"|\"\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ist_index = text.find(\" IST \")\n",
    "        if ist_index != -1:\n",
    "            pipe_index = text.rfind(\"|\", 0, ist_index)  # Search backward for \"|\"\n",
    "\n",
    "            if pipe_index != -1:\n",
    "                return text[:pipe_index].strip() + text[ist_index + len(\" IST \"):].strip()\n",
    "            else:\n",
    "                return text[ist_index + len(\" IST \"):].strip()\n",
    "        return text  # Return original text if \" IST \" is not found\n",
    "    except AttributeError:\n",
    "        return text  # Handle cases where the input might not be a string\n",
    "    \n",
    "# Apply cleaning to the 'text' column in all three dataframes\n",
    "for df in [df_train, df_dev, df_test]:\n",
    "    if not df.empty and 'text' in df.columns:\n",
    "        df['text'] = df['text'].apply(clean_source_info)\n",
    "\n",
    "for df in [df_train, df_dev, df_test]:\n",
    "    if not df.empty and 'text' in df.columns:\n",
    "        df['text'] = df['text'].apply(clean_source_info_2)\n",
    "\n",
    "\n",
    "def remove_parenthetical_expressions(text):\n",
    "    \"\"\"\n",
    "    Remove all text within parentheses () and square brackets []\n",
    "    from the given text.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    # Remove text within parentheses ()\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    \n",
    "    # Remove text within square brackets []\n",
    "    text = re.sub(r'\\[[^\\]]*\\]', '', text)\n",
    "    \n",
    "    # Clean up any double spaces created by the removals\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply cleaning to the 'text' column in all three dataframes\n",
    "for df in [df_train, df_dev, df_test]:\n",
    "    if not df.empty and 'text' in df.columns:\n",
    "        df['text'] = df['text'].apply(remove_parenthetical_expressions)\n",
    "\n",
    "\n",
    "def clean_location_dash(text):\n",
    "    \"\"\"\n",
    "    Clean location information from the beginning of text.\n",
    "    Looks for \"WORD - \" pattern in the first 50 characters and \n",
    "    removes everything from the beginning to the end of the matched pattern.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    # Check only the first 50 characters (or all if less than 50)\n",
    "    first_part = text[:50] if len(text) > 50 else text\n",
    "    \n",
    "    # Pattern to look for location followed by dash: word(s) followed by \" - \"\n",
    "    # This will match patterns like \"DUBLIN - \", \"NEW YORK - \", etc.\n",
    "    pattern = r'^\\w+(\\s+\\w+)* - '\n",
    "    \n",
    "    match = re.search(pattern, first_part)\n",
    "    if match:\n",
    "        return text[match.end():].strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply cleaning to the 'text' column in all three dataframes\n",
    "for df in [df_train, df_dev, df_test]:\n",
    "    if not df.empty and 'text' in df.columns:\n",
    "        df['text'] = df['text'].apply(clean_location_dash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8aa78250-2622-4d2a-a600-dfefa9c62c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'product or material produced', 'manufacturer', 'distributed by', 'industry', 'position held', 'original broadcaster', 'owned by', 'founded by', 'distribution format', 'headquarters location', 'stock exchange', 'currency', 'parent organization', 'chief executive officer', 'director or manager', 'owner of', 'operator', 'member of', 'employer', 'chairperson', 'platform', 'subsidiary', 'legal form', 'publisher', 'developer', 'brand', 'business division', 'location of formation', 'creator'\n"
     ]
    }
   ],
   "source": [
    "# Load and update relations from the file\n",
    "with open('relations.txt', 'r') as file:\n",
    "    relations = [line.replace(\"product/material produced\", \"product or material produced\")\n",
    "                     .replace(\"director/manager\", \"director or manager\") \n",
    "                 for line in file.read().splitlines()]\n",
    "\n",
    "\n",
    "# Convert to formatted string\n",
    "relations_list = ', '.join(f\"'{relation}'\" for relation in relations)\n",
    " \n",
    "print(relations_list)  # Check the updated relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41f0623c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean annotations column\n",
    "\n",
    "def clean_annotations(annotations):  \n",
    "    return [annotation.replace('_', ' ') for annotation in annotations]\n",
    "\n",
    "# Define the function to parse annotations\n",
    "def parse_annotation(annotation_str):\n",
    "    \"\"\"\n",
    "    Parse an annotation string in the format \"entity1 ; entity2 ; relation\"\n",
    "    Returns a tuple (entity1, entity2, relation) or None if parsing fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parts = annotation_str.split(\" ; \")\n",
    "        if len(parts) == 3:\n",
    "            entity1, entity2, relation = parts\n",
    "            return (entity1, entity2, relation)\n",
    "        return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Define the directional relationship pairs\n",
    "relationship_pairs = {\n",
    "    \"product or material produced\": \"manufacturer\",\n",
    "    \"manufacturer\": \"product or material produced\",\n",
    "    \"position held\": \"chief executive officer\",\n",
    "    \"position held\": \"director or manager\",\n",
    "    \"position held\": \"chairperson\",\n",
    "    \"chief executive officer\": \"position held\",\n",
    "    \"director or manager\": \"position held\",\n",
    "    \"chairperson\": \"position held\",\n",
    "    \"owned by\": \"owner of\",\n",
    "    \"owner of\": \"owned by\",\n",
    "    \"parent organization\": \"subsidiary\",\n",
    "    \"subsidiary\": \"parent organization\",\n",
    "    \"creator\": \"founded by\",\n",
    "    \"founded by\": \"creator\"\n",
    "}\n",
    "\n",
    "def process_annotations(annotations):\n",
    "    \"\"\"\n",
    "    Process annotations in the given list of annotations.\n",
    "    This function identifies existing relationships and adds missing reverse relationships.\n",
    "    \"\"\"\n",
    "    existing_relations = set()\n",
    "\n",
    "    # Extract existing relationships\n",
    "    for annotation in annotations:\n",
    "        # Replace \"director_/_manager\" with \"director_or_manager\" before parsing\n",
    "        annotation = annotation.replace(\"director_/_manager\", \"director or manager\")\n",
    "\n",
    "        parsed = parse_annotation(annotation)\n",
    "        if parsed:\n",
    "            existing_relations.add(parsed)\n",
    "\n",
    "    # Identify missing reverse relationships\n",
    "    new_relations = set()\n",
    "    for entity1, entity2, relation in existing_relations:\n",
    "        if relation in relationship_pairs:\n",
    "            reverse_relation = relationship_pairs[relation]\n",
    "            if (entity2, entity1, reverse_relation) not in existing_relations:\n",
    "                new_relations.add((entity2, entity1, reverse_relation))\n",
    "        elif relation in relationship_pairs.values():\n",
    "            # Get all keys that map to this value\n",
    "            reverse_relations = [k for k, v in relationship_pairs.items() if v == relation]\n",
    "            if reverse_relations:\n",
    "                reverse_relation = reverse_relations[0]  # Take the first one\n",
    "                if (entity2, entity1, reverse_relation) not in existing_relations:\n",
    "                    new_relations.add((entity2, entity1, reverse_relation))\n",
    "\n",
    "    # Return all relations combined as a list of formatted strings\n",
    "    return list(annotations) + [\" ; \".join(relation) for relation in new_relations]\n",
    "\n",
    "# Process each DataFrame\n",
    "for df in [df_train, df_dev, df_test]:\n",
    "    if not df.empty and 'annotations' in df.columns:\n",
    "        df['annotations'] = df['annotations'].apply(process_annotations)\n",
    "        df['annotations'] = df['annotations'].apply(clean_annotations)\n",
    "\n",
    "# Function to process annotations into a set of triplets\n",
    "def convert_to_triplet_set(annotation):\n",
    "    if isinstance(annotation, str):  # If stored as a string\n",
    "        annotation = annotation.strip(\"[]\")  # Remove brackets if mistakenly included\n",
    "        annotation = annotation.split(\",\")  # Split into list\n",
    " \n",
    "    if isinstance(annotation, list):  # Ensure it's a list now\n",
    "        triplets = set()\n",
    "        for item in annotation:\n",
    "            parts = [x.strip() for x in item.split(\";\")]  # Split on ';' and remove extra spaces\n",
    "            if len(parts) == 3:  # Only take valid triplets\n",
    "                triplets.add(tuple(parts))\n",
    "        return triplets\n",
    "    return annotation  # Return original if not processable\n",
    " \n",
    "# Apply transformation\n",
    "\n",
    "for df in [df_train, df_dev, df_test]:\n",
    "    df[\"annotations\"] = df[\"annotations\"].apply(convert_to_triplet_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02ef4942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete. Saved as train.jsonl\n",
      "Conversion complete. Saved as validation.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Convert train and validation datatset to JSONL format\n",
    "\n",
    "# Output file paths\n",
    "training_file = \"train.jsonl\"\n",
    "validation_file = \"validation.jsonl\"\n",
    "\n",
    "# Process the training dataset\n",
    "with open(training_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for _, row in df_train.iterrows():\n",
    "        text = row[\"text\"].strip()\n",
    "        triplets = row[\"annotations\"]\n",
    "        \n",
    "        # Convert triplets set to JSON format\n",
    "        triplets_str = \"; \".join([f\"({e1}, {e2}, {rel})\" for e1, e2, rel in triplets])\n",
    "        \n",
    "        # Create JSONL entry\n",
    "        entry = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a relationship extraction model.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Extract relationships from the following text:\\n{text}\"},\n",
    "                {\"role\": \"assistant\", \"content\": triplets_str}\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Write to JSONL file\n",
    "        outfile.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "# Process the validation dataset\n",
    "with open(validation_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for _, row in df_dev.iterrows():\n",
    "        text = row[\"text\"].strip()\n",
    "        triplets = row[\"annotations\"]\n",
    "        \n",
    "        # Convert triplets set to JSON format\n",
    "        triplets_str = \"; \".join([f\"({e1}, {e2}, {rel})\" for e1, e2, rel in triplets])\n",
    "        \n",
    "        # Create JSONL entry\n",
    "        entry = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a relationship extraction model.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Extract relationships from the following text:\\n{text}\"},\n",
    "                {\"role\": \"assistant\", \"content\": triplets_str}\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Write to JSONL file\n",
    "        outfile.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(f\"Conversion complete. Saved as {training_file}\")\n",
    "print(f\"Conversion complete. Saved as {validation_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d7e6f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking train.jsonl:\n",
      "Num examples: 5700\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You are a relationship extraction model.'}\n",
      "{'role': 'user', 'content': 'Extract relationships from the following text:\\nApple Inc Chief Executive Steve Jobs sought to soothe investor concerns about his health on Monday, saying his weight loss was caused by a hormone imbalance that is relatively simple to treat.'}\n",
      "{'role': 'assistant', 'content': '(Apple Inc, Steve Jobs, chief executive officer); (Apple Inc, Steve Jobs, founded by)'}\n",
      "No errors found\n",
      "\n",
      "Checking validation.jsonl:\n",
      "Num examples: 1007\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You are a relationship extraction model.'}\n",
      "{'role': 'user', 'content': 'Extract relationships from the following text:\\nYum China will become a franchise of Yum Brands in Mainland China, the parent of KFC, Pizza Hut and Taco Bell chains said.'}\n",
      "{'role': 'assistant', 'content': '(Taco Bell, Yum Brands, parent organization); (Yum Brands, Pizza Hut, subsidiary); (Yum China, Pizza Hut, brand); (Yum Brands, Pizza Hut, owner of); (Yum Brands, Taco Bell, owner of); (Yum China, Taco Bell, brand); (Pizza Hut, Yum Brands, parent organization); (Yum Brands, Taco Bell, subsidiary)'}\n",
      "No errors found\n",
      "\n",
      "Overall Format Check Summary:\n",
      "Both datasets passed all format checks.\n"
     ]
    }
   ],
   "source": [
    "# Validate JSONL datasets\n",
    "\n",
    "# Data paths\n",
    "training_file = \"train.jsonl\"\n",
    "validation_file = \"validation.jsonl\"\n",
    "\n",
    "# Function to check format errors in a dataset\n",
    "def check_dataset_format(data_path):\n",
    "    # Load the dataset\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        dataset = [json.loads(line) for line in f]\n",
    "\n",
    "    # Initial dataset stats\n",
    "    print(f\"\\nChecking {data_path}:\")\n",
    "    print(\"Num examples:\", len(dataset))\n",
    "    print(\"First example:\")\n",
    "    for message in dataset[0][\"messages\"]:\n",
    "        print(message)\n",
    "\n",
    "    # Format error checks\n",
    "    format_errors = defaultdict(int)\n",
    "\n",
    "    for ex in dataset:\n",
    "        if not isinstance(ex, dict):\n",
    "            format_errors[\"data_type\"] += 1\n",
    "            continue\n",
    "            \n",
    "        messages = ex.get(\"messages\", None)\n",
    "        if not messages:\n",
    "            format_errors[\"missing_messages_list\"] += 1\n",
    "            continue\n",
    "            \n",
    "        for message in messages:\n",
    "            if \"role\" not in message or \"content\" not in message:\n",
    "                format_errors[\"message_missing_key\"] += 1\n",
    "            \n",
    "            if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
    "                format_errors[\"message_unrecognized_key\"] += 1\n",
    "            \n",
    "            if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "                format_errors[\"unrecognized_role\"] += 1\n",
    "                \n",
    "            content = message.get(\"content\", None)\n",
    "            function_call = message.get(\"function_call\", None)\n",
    "            \n",
    "            if (not content and not function_call) or not isinstance(content, str):\n",
    "                format_errors[\"missing_content\"] += 1\n",
    "        \n",
    "        if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "            format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "    if format_errors:\n",
    "        print(\"Found errors:\")\n",
    "        for k, v in format_errors.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "    else:\n",
    "        print(\"No errors found\")\n",
    "    \n",
    "    return format_errors\n",
    "\n",
    "# Check both datasets\n",
    "training_errors = check_dataset_format(training_file)\n",
    "validation_errors = check_dataset_format(validation_file)\n",
    "\n",
    "# Overall summary\n",
    "print(\"\\nOverall Format Check Summary:\")\n",
    "if not training_errors and not validation_errors:\n",
    "    print(\"Both datasets passed all format checks.\")\n",
    "else:\n",
    "    if training_errors:\n",
    "        print(f\"Training dataset ({training_file}) has format errors.\")\n",
    "    else:\n",
    "        print(f\"Training dataset ({training_file}) passed all checks.\")\n",
    "        \n",
    "    if validation_errors:\n",
    "        print(f\"Validation dataset ({validation_file}) has format errors.\")\n",
    "    else:\n",
    "        print(f\"Validation dataset ({validation_file}) passed all checks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f990a6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==================================================\n",
      "Analyzing Training dataset: train.jsonl\n",
      "==================================================\n",
      "Number of examples: 5700\n",
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 37, 1451\n",
      "mean / median: 89.90438596491228, 82.0\n",
      "p5 / p95: 62.0, 114.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 6, 418\n",
      "mean / median: 15.978421052631578, 11.0\n",
      "p5 / p95: 8.0, 30.0\n",
      "\n",
      "0 examples may be over the 16,385 token limit, they will be truncated during fine-tuning\n",
      "\n",
      "Dataset has ~512455 tokens that will be charged for during training\n",
      "By default, we'll train for 3 epochs ~ 1537365 tokens on this dataset\n",
      "\n",
      "\n",
      "==================================================\n",
      "Analyzing Validation dataset: validation.jsonl\n",
      "==================================================\n",
      "Number of examples: 1007\n",
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 37, 1070\n",
      "mean / median: 92.78947368421052, 81.0\n",
      "p5 / p95: 62.0, 118.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 6, 310\n",
      "mean / median: 17.218470705064547, 11.0\n",
      "p5 / p95: 8.0, 33.0\n",
      "\n",
      "0 examples may be over the 16,385 token limit, they will be truncated during fine-tuning\n",
      "\n",
      "Dataset has ~93439 tokens that will be charged for during training\n",
      "By default, we'll train for 3 epochs ~ 280317 tokens on this dataset\n",
      "\n",
      "\n",
      "==================================================\n",
      "COMBINED SUMMARY\n",
      "==================================================\n",
      "Training dataset: 5700 examples, ~512455 tokens\n",
      "Validation dataset: 1007 examples, ~93439 tokens\n",
      "Total billable tokens: ~605894\n",
      "Recommended epochs for training: 3\n",
      "Total examples over token limit: 0\n"
     ]
    }
   ],
   "source": [
    "# Calculate tokens used for finet-tuning\n",
    "\n",
    "# Initialize the tokenizer\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Define token counting functions\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")\n",
    "\n",
    "def analyze_dataset(file_path, dataset_name):\n",
    "    print(f\"\\n\\n{'='*50}\")\n",
    "    print(f\"Analyzing {dataset_name} dataset: {file_path}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        dataset = [json.loads(line) for line in f]\n",
    "    \n",
    "    print(f\"Number of examples: {len(dataset)}\")\n",
    "    \n",
    "    # Warnings and tokens counts\n",
    "    n_missing_system = 0\n",
    "    n_missing_user = 0\n",
    "    n_messages = []\n",
    "    convo_lens = []\n",
    "    assistant_message_lens = []\n",
    "    \n",
    "    for ex in dataset:\n",
    "        messages = ex[\"messages\"]\n",
    "        if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "            n_missing_system += 1\n",
    "        if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "            n_missing_user += 1\n",
    "        n_messages.append(len(messages))\n",
    "        convo_lens.append(num_tokens_from_messages(messages))\n",
    "        assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "        \n",
    "    print(\"Num examples missing system message:\", n_missing_system)\n",
    "    print(\"Num examples missing user message:\", n_missing_user)\n",
    "    print_distribution(n_messages, \"num_messages_per_example\")\n",
    "    print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "    print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "    n_too_long = sum(l > 16385 for l in convo_lens)\n",
    "    print(f\"\\n{n_too_long} examples may be over the 16,385 token limit, they will be truncated during fine-tuning\")\n",
    "    \n",
    "    # Pricing and default n_epochs estimate\n",
    "    MAX_TOKENS_PER_EXAMPLE = 16385\n",
    "    \n",
    "    TARGET_EPOCHS = 3\n",
    "    MIN_TARGET_EXAMPLES = 100\n",
    "    MAX_TARGET_EXAMPLES = 25000\n",
    "    MIN_DEFAULT_EPOCHS = 1\n",
    "    MAX_DEFAULT_EPOCHS = 25\n",
    "    \n",
    "    n_epochs = TARGET_EPOCHS\n",
    "    n_train_examples = len(dataset)\n",
    "    if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "        n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "    elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "        n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "    \n",
    "    n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "    print(f\"\\nDataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "    print(f\"By default, we'll train for {n_epochs} epochs ~ {n_epochs * n_billing_tokens_in_dataset} tokens on this dataset\")\n",
    "    \n",
    "    return {\n",
    "        \"examples\": n_train_examples,\n",
    "        \"billing_tokens\": n_billing_tokens_in_dataset,\n",
    "        \"recommended_epochs\": n_epochs,\n",
    "        \"over_limit_examples\": n_too_long\n",
    "    }\n",
    "\n",
    "# Define file paths\n",
    "training_file = \"train.jsonl\"\n",
    "validation_file = \"validation.jsonl\"\n",
    "\n",
    "# Analyze both datasets\n",
    "train_stats = analyze_dataset(training_file, \"Training\")\n",
    "val_stats = analyze_dataset(validation_file, \"Validation\")\n",
    "\n",
    "# Print combined summary\n",
    "print(\"\\n\\n\" + \"=\"*50)\n",
    "print(\"COMBINED SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Training dataset: {train_stats['examples']} examples, ~{train_stats['billing_tokens']} tokens\")\n",
    "print(f\"Validation dataset: {val_stats['examples']} examples, ~{val_stats['billing_tokens']} tokens\")\n",
    "print(f\"Total billable tokens: ~{train_stats['billing_tokens'] + val_stats['billing_tokens']}\")\n",
    "print(f\"Recommended epochs for training: {train_stats['recommended_epochs']}\")\n",
    "print(f\"Total examples over token limit: {train_stats['over_limit_examples'] + val_stats['over_limit_examples']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90ac919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up API key\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\", API_KEY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f450d07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file-NfhWJpKveT8bR6XE55Kf8U\n",
      "file-Eh1gdyaz8sacCr7VTgXanm\n"
     ]
    }
   ],
   "source": [
    "# Upload JSONL files to server\n",
    "\n",
    "train_set_file = client.files.create(\n",
    "    file = open('train.jsonl', 'rb'),\n",
    "    purpose = \"fine-tune\"\n",
    ")\n",
    "\n",
    "validation_set_file = client.files.create(\n",
    "    file = open('validation.jsonl', 'rb'),\n",
    "    purpose = \"fine-tune\"\n",
    ")\n",
    "\n",
    "print(train_set_file.id)\n",
    "print(validation_set_file.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf01df18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ftjob-4QrG54xwPB1I5ui8L7Sr0lnr\n",
      "None\n",
      "[]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune model\n",
    "\n",
    "response = client.fine_tuning.jobs.create(\n",
    "    training_file=train_set_file.id,\n",
    "    validation_file=validation_set_file.id,\n",
    "    model='gpt-4o-mini-2024-07-18',\n",
    "    hyperparameters={\n",
    "        \"n_epochs\": 3,\n",
    "        \"batch_size\": 16,\n",
    "        \"learning_rate_multiplier\": 0.5\n",
    "    }\n",
    ")\n",
    "print(response.id)\n",
    "print(response.fine_tuned_model)\n",
    "print(response.result_files)\n",
    "print(response.trained_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59184d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-4QrG54xwPB1I5ui8L7Sr0lnr', created_at=1744000431, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-4o-mini-2024-07-18:university-edinburgh::BJYydsxC', finished_at=1744002380, hyperparameters=Hyperparameters(batch_size=16, learning_rate_multiplier=0.5, n_epochs=3), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-uAtoImi2lqXXzOw9bIRsBaOY', result_files=['file-CeUwpBLPUyH5LUsiGpnM3R'], seed=228287464, status='succeeded', trained_tokens=1487295, training_file='file-NfhWJpKveT8bR6XE55Kf8U', validation_file='file-Eh1gdyaz8sacCr7VTgXanm', estimated_finish=None, integrations=[], metadata=None, method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=16, learning_rate_multiplier=0.5, n_epochs=3)), type='supervised'), user_provided_suffix=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check fine-tuning status\n",
    "\n",
    "client.fine_tuning.jobs.retrieve(response.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9b12146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_mean_token_accuracy</th>\n",
       "      <th>train_mean_reward</th>\n",
       "      <th>full_validation_mean_reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6.89892</td>\n",
       "      <td>0.43130</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>6.31564</td>\n",
       "      <td>0.49821</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6.69071</td>\n",
       "      <td>0.47015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6.31620</td>\n",
       "      <td>0.50542</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6.20880</td>\n",
       "      <td>0.48047</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>1065</td>\n",
       "      <td>0.11709</td>\n",
       "      <td>0.95305</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>1066</td>\n",
       "      <td>0.14774</td>\n",
       "      <td>0.93750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>1067</td>\n",
       "      <td>0.09679</td>\n",
       "      <td>0.97318</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>1068</td>\n",
       "      <td>0.15174</td>\n",
       "      <td>0.94663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>1069</td>\n",
       "      <td>0.24491</td>\n",
       "      <td>0.92265</td>\n",
       "      <td>0.32296</td>\n",
       "      <td>0.87161</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1069 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      step  train_loss  train_accuracy  valid_loss  valid_mean_token_accuracy  \\\n",
       "0        1     6.89892         0.43130         NaN                        NaN   \n",
       "1        2     6.31564         0.49821         NaN                        NaN   \n",
       "2        3     6.69071         0.47015         NaN                        NaN   \n",
       "3        4     6.31620         0.50542         NaN                        NaN   \n",
       "4        5     6.20880         0.48047         NaN                        NaN   \n",
       "...    ...         ...             ...         ...                        ...   \n",
       "1064  1065     0.11709         0.95305         NaN                        NaN   \n",
       "1065  1066     0.14774         0.93750         NaN                        NaN   \n",
       "1066  1067     0.09679         0.97318         NaN                        NaN   \n",
       "1067  1068     0.15174         0.94663         NaN                        NaN   \n",
       "1068  1069     0.24491         0.92265     0.32296                    0.87161   \n",
       "\n",
       "      train_mean_reward  full_validation_mean_reward  \n",
       "0                   NaN                          NaN  \n",
       "1                   NaN                          NaN  \n",
       "2                   NaN                          NaN  \n",
       "3                   NaN                          NaN  \n",
       "4                   NaN                          NaN  \n",
       "...                 ...                          ...  \n",
       "1064                NaN                          NaN  \n",
       "1065                NaN                          NaN  \n",
       "1066                NaN                          NaN  \n",
       "1067                NaN                          NaN  \n",
       "1068                NaN                          NaN  \n",
       "\n",
       "[1069 rows x 7 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get fine-tuning results\n",
    "\n",
    "result_files = response.result_files\n",
    "job_id = response.id\n",
    "\n",
    "def get_finetuning_metrics(client, job_id):\n",
    "    def replace_colons_with_underscored(input_string):\n",
    "        return(input_string).replace(':', '_')\n",
    "    \n",
    "    fine_tune_job = client.fine_tuning.jobs.retrieve(job_id)\n",
    "\n",
    "    file_id = fine_tune_job.result_files[0]\n",
    "\n",
    "    response = client.files.content(file_id)\n",
    "\n",
    "    decoded_content = base64.b64decode(response.content).decode('utf-8')\n",
    "\n",
    "    df = pd.read_csv(io.StringIO(decoded_content))\n",
    "\n",
    "    return df\n",
    "\n",
    "get_finetuning_metrics(client, job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df1b25f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning metrics for job: ftjob-4QrG54xwPB1I5ui8L7Sr0lnr\n",
      "--------------------------------------------------------------------------------\n",
      " step  train_loss  train_accuracy\n",
      "    1     6.89892         0.43130\n",
      "    2     6.31564         0.49821\n",
      "    3     6.69071         0.47015\n",
      "    4     6.31620         0.50542\n",
      "    5     6.20880         0.48047\n",
      "    6     5.49890         0.50350\n",
      "    7     5.80474         0.47917\n",
      "    8     5.87262         0.49808\n",
      "    9     5.90369         0.48986\n",
      "   10     5.96327         0.51046\n",
      "   11     6.80829         0.43966\n",
      "   12     5.17892         0.52899\n",
      "   13     5.57006         0.47534\n",
      "   14     5.99666         0.42500\n",
      "   15     3.41044         0.61647\n",
      "   16     4.29579         0.55076\n",
      "   17     5.77769         0.42424\n",
      "   18     4.72079         0.52229\n",
      "   19     4.12150         0.52787\n",
      "   20     5.19358         0.46734\n",
      "   21     4.65668         0.49825\n",
      "   22     3.65729         0.56209\n",
      "   23     3.66805         0.57771\n",
      "   24     3.35591         0.59245\n",
      "   25     3.85831         0.56972\n",
      "   26     3.66687         0.56287\n",
      "   27     3.65861         0.59696\n",
      "   28     1.77804         0.73375\n",
      "   29     3.59638         0.55285\n",
      "   30     2.50183         0.60916\n",
      "   31     3.29893         0.59000\n",
      "   32     2.94912         0.60455\n",
      "   33     2.59208         0.67033\n",
      "   34     2.81457         0.61765\n",
      "   35     2.41644         0.64207\n",
      "   36     2.48202         0.60985\n",
      "   37     2.41302         0.66505\n",
      "   38     2.51619         0.61278\n",
      "   39     2.06874         0.64784\n",
      "   40     2.13486         0.65613\n",
      "   41     2.23778         0.62400\n",
      "   42     2.12719         0.61633\n",
      "   43     1.42719         0.66942\n",
      "   44     1.46487         0.66892\n",
      "   45     1.61607         0.66541\n",
      "   46     1.79363         0.63137\n",
      "   47     1.41360         0.69091\n",
      "   48     1.42429         0.70711\n",
      "   49     1.49614         0.63008\n",
      "   50     1.25602         0.71186\n",
      "   51     1.47748         0.63396\n",
      "   52     0.98637         0.75419\n",
      "   53     1.26636         0.65975\n",
      "   54     1.14007         0.72347\n",
      "   55     1.12469         0.72125\n",
      "   56     0.93809         0.71691\n",
      "   57     1.00696         0.74194\n",
      "   58     0.78750         0.75852\n",
      "   59     1.03142         0.74552\n",
      "   60     0.95505         0.75124\n",
      "   61     0.99612         0.75309\n",
      "   62     0.69204         0.78746\n",
      "   63     0.63625         0.78641\n",
      "   64     0.64230         0.82353\n",
      "   65     0.89900         0.77037\n",
      "   66     0.72738         0.76250\n",
      "   67     0.57919         0.82484\n",
      "   68     0.77705         0.80645\n",
      "   69     0.73069         0.79893\n",
      "   70     0.80792         0.79741\n",
      "   71     0.63575         0.79023\n",
      "   72     0.63268         0.82470\n",
      "   73     0.69886         0.81763\n",
      "   74     0.65333         0.83200\n",
      "   75     0.46359         0.84444\n",
      "   76     0.71496         0.81624\n",
      "   77     0.73934         0.79876\n",
      "   78     0.66083         0.82490\n",
      "   79     0.53509         0.82609\n",
      "   80     0.41691         0.85142\n",
      "   81     0.49336         0.83193\n",
      "   82     0.68865         0.81172\n",
      "   83     0.40526         0.88031\n",
      "   84     0.52638         0.82960\n",
      "   85     0.39876         0.85667\n",
      "   86     0.52964         0.80806\n",
      "   87     0.47976         0.83379\n",
      "   88     0.46640         0.83461\n",
      "   89     0.47470         0.83566\n",
      "   90     0.49180         0.83571\n",
      "   91     0.64070         0.80970\n",
      "   92     0.61092         0.82392\n",
      "   93     0.45837         0.85075\n",
      "   94     0.45015         0.85537\n",
      "   95     0.37945         0.86818\n",
      "   96     0.30505         0.90584\n",
      "   97     0.45094         0.83226\n",
      "   98     0.57874         0.83282\n",
      "   99     0.61044         0.81525\n",
      "  100     0.49913         0.87500\n",
      "  101     0.42168         0.84103\n",
      "  102     0.51226         0.83882\n",
      "  103     0.51363         0.85246\n",
      "  104     0.56805         0.81994\n",
      "  105     0.37099         0.85754\n",
      "  106     0.51621         0.84871\n",
      "  107     0.46324         0.83375\n",
      "  108     0.45171         0.84226\n",
      "  109     0.67703         0.79817\n",
      "  110     0.54890         0.84167\n",
      "  111     0.51272         0.84681\n",
      "  112     0.51160         0.82493\n",
      "  113     0.40239         0.85965\n",
      "  114     0.40841         0.87361\n",
      "  115     0.39645         0.87295\n",
      "  116     0.46961         0.84615\n",
      "  117     0.35940         0.87549\n",
      "  118     0.43197         0.86695\n",
      "  119     0.42568         0.84706\n",
      "  120     0.49183         0.86624\n",
      "  121     0.40220         0.86879\n",
      "  122     0.41094         0.85271\n",
      "  123     0.43681         0.87069\n",
      "  124     0.32683         0.87791\n",
      "  125     0.54419         0.82463\n",
      "  126     0.41404         0.85246\n",
      "  127     0.32632         0.89641\n",
      "  128     0.32056         0.91250\n",
      "  129     0.48007         0.84722\n",
      "  130     0.30368         0.90614\n",
      "  131     0.47818         0.84520\n",
      "  132     0.42052         0.86901\n",
      "  133     0.37974         0.89610\n",
      "  134     0.28009         0.91266\n",
      "  135     0.26767         0.91903\n",
      "  136     0.50603         0.85764\n",
      "  137     0.31098         0.89041\n",
      "  138     0.48116         0.86792\n",
      "  139     0.38099         0.86348\n",
      "  140     0.30215         0.89324\n",
      "  141     0.39268         0.84810\n",
      "  142     0.33112         0.89073\n",
      "  143     0.48223         0.86000\n",
      "  144     0.31830         0.87328\n",
      "  145     0.37482         0.84530\n",
      "  146     0.48462         0.86038\n",
      "  147     0.44432         0.86957\n",
      "  148     0.43477         0.87361\n",
      "  149     0.32216         0.88966\n",
      "  150     0.31175         0.90074\n",
      "  151     0.44733         0.88218\n",
      "  152     0.34097         0.87805\n",
      "  153     0.37425         0.85580\n",
      "  154     0.38934         0.86780\n",
      "  155     0.35905         0.90361\n",
      "  156     0.33681         0.88095\n",
      "  157     0.47555         0.87037\n",
      "  158     0.46345         0.86134\n",
      "  159     0.49196         0.84859\n",
      "  160     0.46630         0.89407\n",
      "  161     0.34088         0.90421\n",
      "  162     0.31908         0.89388\n",
      "  163     0.40211         0.87421\n",
      "  164     0.42208         0.90763\n",
      "  165     0.35457         0.87692\n",
      "  166     0.41347         0.86531\n",
      "  167     0.32015         0.90722\n",
      "  168     0.36399         0.85921\n",
      "  169     0.41873         0.86770\n",
      "  170     0.41929         0.87045\n",
      "  171     0.28171         0.92593\n",
      "  172     0.36176         0.88514\n",
      "  173     0.33466         0.88032\n",
      "  174     0.29951         0.89869\n",
      "  175     0.40548         0.86799\n",
      "  176     0.41621         0.88031\n",
      "  177     0.41780         0.86942\n",
      "  178     0.31821         0.89700\n",
      "  179     0.30757         0.89420\n",
      "  180     0.33487         0.89818\n",
      "  181     0.34015         0.89394\n",
      "  182     0.33917         0.88353\n",
      "  183     0.37403         0.86885\n",
      "  184     0.40987         0.85447\n",
      "  185     0.29854         0.89540\n",
      "  186     0.55727         0.83489\n",
      "  187     0.44117         0.82817\n",
      "  188     0.25136         0.91034\n",
      "  189     0.22232         0.91054\n",
      "  190     0.23948         0.92171\n",
      "  191     0.26850         0.92213\n",
      "  192     0.25235         0.91144\n",
      "  193     0.25363         0.90288\n",
      "  194     0.35096         0.90572\n",
      "  195     0.36523         0.87037\n",
      "  196     0.22055         0.93704\n",
      "  197     0.30948         0.87665\n",
      "  198     0.37823         0.87709\n",
      "  199     0.35195         0.87333\n",
      "  200     0.46871         0.86149\n",
      "  201     0.35911         0.86266\n",
      "  202     0.31671         0.88554\n",
      "  203     0.50532         0.86531\n",
      "  204     0.30388         0.87333\n",
      "  205     0.24808         0.91864\n",
      "  206     0.21738         0.92887\n",
      "  207     0.35870         0.88930\n",
      "  208     0.33256         0.88889\n",
      "  209     0.36083         0.85445\n",
      "  210     0.25001         0.89113\n",
      "  211     0.39568         0.86711\n",
      "  212     0.39797         0.88646\n",
      "  213     0.44152         0.88929\n",
      "  214     0.20773         0.91866\n",
      "  215     0.19862         0.92126\n",
      "  216     0.30415         0.88820\n",
      "  217     0.28585         0.89520\n",
      "  218     0.34193         0.89189\n",
      "  219     0.30776         0.89062\n",
      "  220     0.25783         0.93249\n",
      "  221     0.30959         0.86982\n",
      "  222     0.24790         0.91844\n",
      "  223     0.37200         0.88031\n",
      "  224     0.32883         0.89761\n",
      "  225     0.34561         0.87549\n",
      "  226     0.34478         0.88538\n",
      "  227     0.27486         0.89807\n",
      "  228     0.29681         0.89298\n",
      "  229     0.38891         0.88390\n",
      "  230     0.27392         0.91235\n",
      "  231     0.43269         0.86402\n",
      "  232     0.43101         0.88559\n",
      "  233     0.26285         0.90583\n",
      "  234     0.38745         0.87065\n",
      "  235     0.47611         0.87281\n",
      "  236     0.37870         0.87838\n",
      "  237     0.33301         0.89455\n",
      "  238     0.32776         0.89583\n",
      "  239     0.36381         0.89956\n",
      "  240     0.35084         0.89916\n",
      "  241     0.31615         0.86503\n",
      "  242     0.40352         0.88000\n",
      "  243     0.38424         0.88382\n",
      "  244     0.38569         0.85926\n",
      "  245     0.30389         0.89860\n",
      "  246     0.28302         0.90071\n",
      "  247     0.38185         0.89020\n",
      "  248     0.46153         0.87204\n",
      "  249     0.38224         0.88163\n",
      "  250     0.35817         0.87559\n",
      "  251     0.28675         0.91892\n",
      "  252     0.34213         0.89121\n",
      "  253     0.19876         0.93007\n",
      "  254     0.21860         0.92557\n",
      "  255     0.25323         0.91139\n",
      "  256     0.26822         0.90947\n",
      "  257     0.28624         0.89455\n",
      "  258     0.21821         0.91367\n",
      "  259     0.26139         0.91121\n",
      "  260     0.37653         0.87608\n",
      "  261     0.32094         0.90119\n",
      "  262     0.25090         0.90645\n",
      "  263     0.30463         0.88716\n",
      "  264     0.26266         0.89351\n",
      "  265     0.29289         0.89809\n",
      "  266     0.22717         0.92188\n",
      "  267     0.39467         0.87187\n",
      "  268     0.25686         0.90541\n",
      "  269     0.18825         0.93357\n",
      "  270     0.30865         0.88809\n",
      "  271     0.32514         0.92202\n",
      "  272     0.21954         0.92527\n",
      "  273     0.21366         0.92226\n",
      "  274     0.29809         0.90270\n",
      "  275     0.24406         0.92000\n",
      "  276     0.14539         0.96923\n",
      "  277     0.18283         0.93697\n",
      "  278     0.26195         0.90873\n",
      "  279     0.34476         0.89593\n",
      "  280     0.19159         0.92278\n",
      "  281     0.39874         0.86006\n",
      "  282     0.52590         0.87019\n",
      "  283     0.40969         0.86411\n",
      "  284     0.28634         0.90461\n",
      "  285     0.32190         0.89219\n",
      "  286     0.31211         0.92952\n",
      "  287     0.40346         0.85799\n",
      "  288     0.30278         0.89905\n",
      "  289     0.34707         0.86389\n",
      "  290     0.24692         0.92623\n",
      "  291     0.20806         0.91837\n",
      "  292     0.27305         0.90690\n",
      "  293     0.31779         0.91584\n",
      "  294     0.33783         0.87421\n",
      "  295     0.27393         0.90282\n",
      "  296     0.31097         0.91031\n",
      "  297     0.27580         0.90370\n",
      "  298     0.21996         0.93569\n",
      "  299     0.31952         0.89069\n",
      "  300     0.52285         0.83843\n",
      "  301     0.36035         0.86972\n",
      "  302     0.33411         0.89508\n",
      "  303     0.19896         0.92923\n",
      "  304     0.29915         0.88396\n",
      "  305     0.33261         0.90083\n",
      "  306     0.26737         0.92430\n",
      "  307     0.42932         0.85934\n",
      "  308     0.33579         0.89667\n",
      "  309     0.25207         0.90556\n",
      "  310     0.32264         0.88957\n",
      "  311     0.30583         0.88803\n",
      "  312     0.31590         0.92344\n",
      "  313     0.33136         0.91165\n",
      "  314     0.20939         0.91603\n",
      "  315     0.36593         0.86166\n",
      "  316     0.29654         0.91156\n",
      "  317     0.26902         0.91775\n",
      "  318     0.27518         0.89734\n",
      "  319     0.27655         0.89286\n",
      "  320     0.20061         0.91866\n",
      "  321     0.34819         0.88615\n",
      "  322     0.21906         0.93680\n",
      "  323     0.30576         0.88722\n",
      "  324     0.25164         0.90535\n",
      "  325     0.38527         0.90141\n",
      "  326     0.21291         0.92333\n",
      "  327     0.40235         0.84543\n",
      "  328     0.24732         0.91096\n",
      "  329     0.22899         0.91928\n",
      "  330     0.29542         0.90805\n",
      "  331     0.25884         0.90476\n",
      "  332     0.30641         0.90368\n",
      "  333     0.20134         0.94222\n",
      "  334     0.31104         0.89531\n",
      "  335     0.20773         0.92369\n",
      "  336     0.21240         0.93220\n",
      "  337     0.26321         0.92241\n",
      "  338     0.30068         0.90071\n",
      "  339     0.24282         0.89209\n",
      "  340     0.28739         0.89147\n",
      "  341     0.25749         0.90222\n",
      "  342     0.28182         0.90354\n",
      "  343     0.28452         0.92000\n",
      "  344     0.23193         0.91795\n",
      "  345     0.27527         0.90775\n",
      "  346     0.28954         0.90217\n",
      "  347     0.30793         0.88529\n",
      "  348     0.32139         0.89098\n",
      "  349     0.24326         0.91489\n",
      "  350     0.28951         0.89630\n",
      "  351     0.25332         0.90299\n",
      "  352     0.38037         0.88123\n",
      "  353     0.23332         0.94656\n",
      "  354     0.23716         0.90385\n",
      "  355     0.24191         0.91592\n",
      "  356     0.23808         0.91362\n",
      "  357     0.31128         0.89377\n",
      "  358     0.21901         0.92075\n",
      "  359     0.24725         0.89590\n",
      "  360     0.18728         0.92641\n",
      "  361     0.21213         0.92409\n",
      "  362     0.26635         0.90775\n",
      "  363     0.26500         0.89532\n",
      "  364     0.20679         0.91349\n",
      "  365     0.19751         0.95536\n",
      "  366     0.32955         0.90210\n",
      "  367     0.23680         0.90960\n",
      "  368     0.22314         0.91892\n",
      "  369     0.27146         0.89298\n",
      "  370     0.36880         0.93033\n",
      "  371     0.16632         0.92898\n",
      "  372     0.30895         0.89338\n",
      "  373     0.32093         0.88077\n",
      "  374     0.24895         0.91468\n",
      "  375     0.19754         0.93443\n",
      "  376     0.28472         0.89905\n",
      "  377     0.26466         0.91515\n",
      "  378     0.29215         0.90187\n",
      "  379     0.23659         0.90795\n",
      "  380     0.25944         0.91538\n",
      "  381     0.37456         0.88177\n",
      "  382     0.32363         0.91304\n",
      "  383     0.28514         0.87805\n",
      "  384     0.39623         0.88194\n",
      "  385     0.27158         0.89567\n",
      "  386     0.30118         0.88759\n",
      "  387     0.17780         0.95089\n",
      "  388     0.30088         0.90647\n",
      "  389     0.23391         0.91803\n",
      "  390     0.22244         0.91597\n",
      "  391     0.34650         0.87967\n",
      "  392     0.21444         0.91406\n",
      "  393     0.25929         0.90947\n",
      "  394     0.28997         0.90838\n",
      "  395     0.25189         0.91406\n",
      "  396     0.25166         0.91615\n",
      "  397     0.25907         0.91304\n",
      "  398     0.31484         0.88976\n",
      "  399     0.27476         0.89037\n",
      "  400     0.17882         0.94186\n",
      "  401     0.18629         0.92675\n",
      "  402     0.22807         0.93701\n",
      "  403     0.31890         0.88433\n",
      "  404     0.26645         0.90947\n",
      "  405     0.21238         0.92881\n",
      "  406     0.31434         0.87594\n",
      "  407     0.21421         0.92746\n",
      "  408     0.34087         0.90741\n",
      "  409     0.23063         0.92500\n",
      "  410     0.28774         0.87202\n",
      "  411     0.29095         0.89655\n",
      "  412     0.34141         0.89892\n",
      "  413     0.31715         0.88793\n",
      "  414     0.19580         0.92889\n",
      "  415     0.29787         0.89950\n",
      "  416     0.18068         0.92105\n",
      "  417     0.21936         0.92262\n",
      "  418     0.21875         0.93633\n",
      "  419     0.15586         0.96124\n",
      "  420     0.34380         0.89455\n",
      "  421     0.20405         0.92698\n",
      "  422     0.21792         0.92609\n",
      "  423     0.32367         0.87869\n",
      "  424     0.25745         0.90391\n",
      "  425     0.26418         0.90335\n",
      "  426     0.17952         0.94574\n",
      "  427     0.23338         0.92966\n",
      "  428     0.24623         0.92469\n",
      "  429     0.22948         0.93213\n",
      "  430     0.39452         0.87295\n",
      "  431     0.19950         0.93822\n",
      "  432     0.13470         0.94667\n",
      "  433     0.18207         0.94118\n",
      "  434     0.18329         0.92857\n",
      "  435     0.18645         0.93258\n",
      "  436     0.16613         0.94815\n",
      "  437     0.18432         0.94224\n",
      "  438     0.23437         0.92373\n",
      "  439     0.23392         0.91343\n",
      "  440     0.26327         0.89450\n",
      "  441     0.23975         0.92262\n",
      "  442     0.21933         0.92278\n",
      "  443     0.19525         0.94030\n",
      "  444     0.21252         0.92511\n",
      "  445     0.30527         0.89490\n",
      "  446     0.28821         0.89542\n",
      "  447     0.12352         0.96979\n",
      "  448     0.18627         0.92908\n",
      "  449     0.34782         0.88333\n",
      "  450     0.24385         0.90909\n",
      "  451     0.23390         0.90746\n",
      "  452     0.21495         0.93060\n",
      "  453     0.15222         0.96139\n",
      "  454     0.13350         0.95378\n",
      "  455     0.19165         0.92478\n",
      "  456     0.27991         0.91416\n",
      "  457     0.16587         0.93266\n",
      "  458     0.22353         0.91321\n",
      "  459     0.16794         0.96170\n",
      "  460     0.26711         0.90452\n",
      "  461     0.40897         0.89113\n",
      "  462     0.11006         0.96653\n",
      "  463     0.22072         0.91716\n",
      "  464     0.26103         0.91497\n",
      "  465     0.21643         0.92615\n",
      "  466     0.34990         0.90033\n",
      "  467     0.30324         0.89437\n",
      "  468     0.20535         0.91727\n",
      "  469     0.14350         0.96715\n",
      "  470     0.18873         0.93878\n",
      "  471     0.25941         0.91593\n",
      "  472     0.15364         0.94064\n",
      "  473     0.31748         0.88636\n",
      "  474     0.24317         0.88963\n",
      "  475     0.22390         0.92339\n",
      "  476     0.29583         0.92609\n",
      "  477     0.18658         0.93657\n",
      "  478     0.24009         0.90062\n",
      "  479     0.16077         0.93688\n",
      "  480     0.16687         0.94909\n",
      "  481     0.15233         0.95238\n",
      "  482     0.08561         0.97490\n",
      "  483     0.13766         0.95455\n",
      "  484     0.12250         0.95215\n",
      "  485     0.24925         0.90303\n",
      "  486     0.22378         0.90588\n",
      "  487     0.14019         0.95341\n",
      "  488     0.23828         0.93281\n",
      "  489     0.14077         0.96454\n",
      "  490     0.30340         0.90432\n",
      "  491     0.26612         0.90000\n",
      "  492     0.29874         0.89922\n",
      "  493     0.22934         0.94366\n",
      "  494     0.17474         0.94430\n",
      "  495     0.28189         0.90714\n",
      "  496     0.33734         0.88832\n",
      "  497     0.26114         0.89914\n",
      "  498     0.23606         0.90204\n",
      "  499     0.25855         0.89895\n",
      "  500     0.17842         0.94386\n",
      "  501     0.32238         0.89888\n",
      "  502     0.17152         0.94170\n",
      "  503     0.24470         0.90671\n",
      "  504     0.32493         0.91262\n",
      "  505     0.25141         0.91085\n",
      "  506     0.25408         0.90217\n",
      "  507     0.27267         0.92409\n",
      "  508     0.44576         0.89754\n",
      "  509     0.29896         0.88750\n",
      "  510     0.25599         0.91291\n",
      "  511     0.28341         0.91785\n",
      "  512     0.20824         0.93976\n",
      "  513     0.24714         0.92453\n",
      "  514     0.30096         0.88930\n",
      "  515     0.39044         0.88968\n",
      "  516     0.19686         0.92038\n",
      "  517     0.32096         0.91080\n",
      "  518     0.19735         0.93448\n",
      "  519     0.15074         0.95257\n",
      "  520     0.22532         0.91964\n",
      "  521     0.20096         0.93850\n",
      "  522     0.25248         0.92000\n",
      "  523     0.19874         0.91781\n",
      "  524     0.20190         0.93562\n",
      "  525     0.15161         0.95902\n",
      "  526     0.30071         0.89753\n",
      "  527     0.20331         0.94485\n",
      "  528     0.17392         0.92377\n",
      "  529     0.22187         0.92116\n",
      "  530     0.17813         0.94030\n",
      "  531     0.12816         0.95964\n",
      "  532     0.13659         0.95305\n",
      "  533     0.13885         0.95192\n",
      "  534     0.20341         0.93077\n",
      "  535     0.16914         0.94561\n",
      "  536     0.26127         0.92952\n",
      "  537     0.27219         0.91246\n",
      "  538     0.22537         0.91216\n",
      "  539     0.26615         0.93220\n",
      "  540     0.17627         0.93413\n",
      "  541     0.37165         0.88380\n",
      "  542     0.32014         0.89216\n",
      "  543     0.27998         0.92105\n",
      "  544     0.18186         0.92254\n",
      "  545     0.19690         0.93333\n",
      "  546     0.22305         0.91941\n",
      "  547     0.27470         0.91033\n",
      "  548     0.17992         0.93569\n",
      "  549     0.29651         0.93061\n",
      "  550     0.16967         0.93393\n",
      "  551     0.30408         0.89298\n",
      "  552     0.17816         0.95440\n",
      "  553     0.20317         0.93080\n",
      "  554     0.26368         0.92784\n",
      "  555     0.16086         0.93860\n",
      "  556     0.16033         0.96141\n",
      "  557     0.17684         0.95088\n",
      "  558     0.15922         0.94052\n",
      "  559     0.31660         0.88136\n",
      "  560     0.24210         0.92368\n",
      "  561     0.34944         0.87057\n",
      "  562     0.35319         0.90184\n",
      "  563     0.21083         0.92607\n",
      "  564     0.29075         0.90171\n",
      "  565     0.26495         0.91373\n",
      "  566     0.26249         0.90678\n",
      "  567     0.16680         0.94231\n",
      "  568     0.32268         0.89372\n",
      "  569     0.25314         0.90361\n",
      "  570     0.33000         0.87063\n",
      "  571     0.23349         0.90377\n",
      "  572     0.16236         0.94371\n",
      "  573     0.16671         0.93277\n",
      "  574     0.22734         0.91558\n",
      "  575     0.38335         0.88106\n",
      "  576     0.20041         0.93667\n",
      "  577     0.28230         0.91188\n",
      "  578     0.38150         0.87833\n",
      "  579     0.25622         0.90826\n",
      "  580     0.16425         0.95000\n",
      "  581     0.23043         0.92926\n",
      "  582     0.17615         0.93358\n",
      "  583     0.24272         0.90141\n",
      "  584     0.15150         0.94253\n",
      "  585     0.21689         0.94248\n",
      "  586     0.34586         0.89723\n",
      "  587     0.20671         0.92917\n",
      "  588     0.17354         0.95912\n",
      "  589     0.21928         0.92490\n",
      "  590     0.21885         0.93722\n",
      "  591     0.17641         0.92527\n",
      "  592     0.30814         0.90323\n",
      "  593     0.14757         0.94030\n",
      "  594     0.19326         0.93141\n",
      "  595     0.19911         0.94425\n",
      "  596     0.21181         0.90728\n",
      "  597     0.18202         0.94020\n",
      "  598     0.30232         0.90087\n",
      "  599     0.15325         0.95565\n",
      "  600     0.24599         0.91525\n",
      "  601     0.33424         0.91471\n",
      "  602     0.23189         0.92378\n",
      "  603     0.21716         0.92093\n",
      "  604     0.16758         0.95254\n",
      "  605     0.21144         0.90989\n",
      "  606     0.31079         0.89619\n",
      "  607     0.30878         0.93213\n",
      "  608     0.25499         0.91436\n",
      "  609     0.20350         0.94340\n",
      "  610     0.15807         0.94258\n",
      "  611     0.20990         0.93226\n",
      "  612     0.18858         0.94421\n",
      "  613     0.19863         0.91845\n",
      "  614     0.19857         0.91968\n",
      "  615     0.45904         0.87983\n",
      "  616     0.26020         0.90780\n",
      "  617     0.38369         0.87425\n",
      "  618     0.18529         0.93478\n",
      "  619     0.15967         0.95430\n",
      "  620     0.21098         0.92925\n",
      "  621     0.13946         0.95000\n",
      "  622     0.23055         0.92212\n",
      "  623     0.21784         0.92627\n",
      "  624     0.26473         0.91743\n",
      "  625     0.20374         0.94424\n",
      "  626     0.14944         0.95122\n",
      "  627     0.34794         0.86395\n",
      "  628     0.21981         0.91753\n",
      "  629     0.18602         0.94667\n",
      "  630     0.21634         0.94142\n",
      "  631     0.31225         0.89837\n",
      "  632     0.25436         0.90588\n",
      "  633     0.21628         0.92308\n",
      "  634     0.13670         0.97357\n",
      "  635     0.22152         0.93644\n",
      "  636     0.21075         0.92857\n",
      "  637     0.17676         0.93436\n",
      "  638     0.34555         0.88589\n",
      "  639     0.19860         0.90789\n",
      "  640     0.15907         0.93727\n",
      "  641     0.10274         0.97059\n",
      "  642     0.11402         0.95956\n",
      "  643     0.25471         0.91525\n",
      "  644     0.16351         0.93776\n",
      "  645     0.24014         0.91722\n",
      "  646     0.17704         0.93793\n",
      "  647     0.16403         0.94425\n",
      "  648     0.15616         0.94323\n",
      "  649     0.21858         0.92000\n",
      "  650     0.24007         0.89735\n",
      "  651     0.13166         0.94982\n",
      "  652     0.18762         0.93886\n",
      "  653     0.22014         0.92045\n",
      "  654     0.29019         0.88060\n",
      "  655     0.13728         0.95374\n",
      "  656     0.15813         0.92823\n",
      "  657     0.21023         0.95714\n",
      "  658     0.14167         0.95259\n",
      "  659     0.13490         0.94444\n",
      "  660     0.20121         0.92364\n",
      "  661     0.17552         0.94198\n",
      "  662     0.11444         0.96053\n",
      "  663     0.23029         0.94894\n",
      "  664     0.25338         0.88983\n",
      "  665     0.28576         0.88525\n",
      "  666     0.25921         0.91667\n",
      "  667     0.28066         0.91489\n",
      "  668     0.11840         0.96820\n",
      "  669     0.33403         0.90862\n",
      "  670     0.12015         0.96203\n",
      "  671     0.16010         0.94118\n",
      "  672     0.16394         0.92557\n",
      "  673     0.20180         0.93725\n",
      "  674     0.19575         0.93684\n",
      "  675     0.19089         0.95021\n",
      "  676     0.17847         0.93617\n",
      "  677     0.19116         0.92308\n",
      "  678     0.12342         0.94361\n",
      "  679     0.26731         0.89946\n",
      "  680     0.23890         0.91733\n",
      "  681     0.23740         0.94348\n",
      "  682     0.22071         0.93810\n",
      "  683     0.16415         0.95806\n",
      "  684     0.18492         0.92857\n",
      "  685     0.24500         0.93210\n",
      "  686     0.18291         0.93717\n",
      "  687     0.19619         0.92605\n",
      "  688     0.30983         0.88957\n",
      "  689     0.18788         0.92636\n",
      "  690     0.28404         0.90136\n",
      "  691     0.23038         0.92880\n",
      "  692     0.21791         0.92437\n",
      "  693     0.20903         0.94144\n",
      "  694     0.24194         0.90726\n",
      "  695     0.20871         0.92834\n",
      "  696     0.26428         0.92409\n",
      "  697     0.17731         0.94444\n",
      "  698     0.18631         0.94222\n",
      "  699     0.09068         0.96601\n",
      "  700     0.19994         0.93380\n",
      "  701     0.31830         0.89820\n",
      "  702     0.18312         0.95219\n",
      "  703     0.23286         0.92339\n",
      "  704     0.14554         0.93443\n",
      "  705     0.16382         0.94924\n",
      "  706     0.16262         0.91983\n",
      "  707     0.29795         0.88302\n",
      "  708     0.13697         0.95726\n",
      "  709     0.13880         0.94118\n",
      "  710     0.20944         0.92332\n",
      "  711     0.23979         0.89781\n",
      "  712     0.24967         0.92308\n",
      "  713     0.22051         0.92727\n",
      "  714     0.17320         0.93478\n",
      "  715     0.21318         0.93436\n",
      "  716     0.16670         0.95307\n",
      "  717     0.28031         0.88368\n",
      "  718     0.24282         0.92952\n",
      "  719     0.18217         0.92500\n",
      "  720     0.11400         0.95982\n",
      "  721     0.15497         0.93426\n",
      "  722     0.15267         0.93515\n",
      "  723     0.11154         0.97643\n",
      "  724     0.16468         0.92798\n",
      "  725     0.19360         0.93201\n",
      "  726     0.20115         0.93548\n",
      "  727     0.17378         0.93805\n",
      "  728     0.14466         0.95276\n",
      "  729     0.20009         0.93091\n",
      "  730     0.20230         0.93396\n",
      "  731     0.10529         0.97351\n",
      "  732     0.15236         0.93510\n",
      "  733     0.13607         0.95341\n",
      "  734     0.11501         0.95868\n",
      "  735     0.21826         0.94444\n",
      "  736     0.19822         0.92157\n",
      "  737     0.18740         0.92281\n",
      "  738     0.26042         0.91979\n",
      "  739     0.11853         0.96923\n",
      "  740     0.11706         0.97125\n",
      "  741     0.13372         0.95513\n",
      "  742     0.06913         0.98814\n",
      "  743     0.15619         0.96098\n",
      "  744     0.16882         0.94444\n",
      "  745     0.10413         0.96500\n",
      "  746     0.24302         0.93893\n",
      "  747     0.23348         0.91736\n",
      "  748     0.25475         0.91259\n",
      "  749     0.20243         0.95041\n",
      "  750     0.25193         0.92308\n",
      "  751     0.21359         0.93015\n",
      "  752     0.26481         0.92593\n",
      "  753     0.24079         0.90751\n",
      "  754     0.13562         0.94864\n",
      "  755     0.15707         0.93709\n",
      "  756     0.17233         0.93226\n",
      "  757     0.10255         0.96311\n",
      "  758     0.12243         0.96013\n",
      "  759     0.13658         0.94643\n",
      "  760     0.12437         0.94556\n",
      "  761     0.21425         0.93035\n",
      "  762     0.16685         0.93464\n",
      "  763     0.22208         0.92434\n",
      "  764     0.21842         0.94419\n",
      "  765     0.15170         0.95149\n",
      "  766     0.15624         0.95547\n",
      "  767     0.12500         0.94828\n",
      "  768     0.28820         0.91304\n",
      "  769     0.26730         0.91167\n",
      "  770     0.28281         0.90909\n",
      "  771     0.18808         0.95070\n",
      "  772     0.20590         0.94030\n",
      "  773     0.12271         0.95703\n",
      "  774     0.18009         0.93657\n",
      "  775     0.17440         0.92910\n",
      "  776     0.13395         0.95455\n",
      "  777     0.20477         0.92857\n",
      "  778     0.13946         0.94882\n",
      "  779     0.25552         0.93191\n",
      "  780     0.21789         0.93363\n",
      "  781     0.27865         0.90400\n",
      "  782     0.17134         0.93968\n",
      "  783     0.23721         0.93050\n",
      "  784     0.13096         0.95549\n",
      "  785     0.09850         0.96443\n",
      "  786     0.12702         0.95702\n",
      "  787     0.15117         0.95584\n",
      "  788     0.23849         0.94667\n",
      "  789     0.22188         0.94068\n",
      "  790     0.12702         0.96170\n",
      "  791     0.15373         0.93855\n",
      "  792     0.22972         0.91793\n",
      "  793     0.21655         0.93248\n",
      "  794     0.12618         0.94984\n",
      "  795     0.15043         0.94065\n",
      "  796     0.19462         0.93215\n",
      "  797     0.19238         0.94672\n",
      "  798     0.14108         0.95161\n",
      "  799     0.24004         0.93385\n",
      "  800     0.15285         0.93953\n",
      "  801     0.14656         0.94907\n",
      "  802     0.14419         0.96313\n",
      "  803     0.13733         0.96186\n",
      "  804     0.21418         0.94273\n",
      "  805     0.11037         0.95455\n",
      "  806     0.22703         0.92082\n",
      "  807     0.27900         0.89681\n",
      "  808     0.24100         0.92083\n",
      "  809     0.15386         0.94850\n",
      "  810     0.28889         0.91697\n",
      "  811     0.22160         0.92176\n",
      "  812     0.22493         0.93416\n",
      "  813     0.28661         0.91803\n",
      "  814     0.19878         0.91304\n",
      "  815     0.20787         0.93189\n",
      "  816     0.11954         0.96517\n",
      "  817     0.15060         0.94348\n",
      "  818     0.17657         0.92327\n",
      "  819     0.11352         0.96847\n",
      "  820     0.16956         0.93704\n",
      "  821     0.11270         0.96403\n",
      "  822     0.21520         0.92340\n",
      "  823     0.19838         0.93103\n",
      "  824     0.19065         0.95720\n",
      "  825     0.17117         0.92517\n",
      "  826     0.30800         0.93656\n",
      "  827     0.14430         0.94881\n",
      "  828     0.12410         0.95238\n",
      "  829     0.17045         0.94249\n",
      "  830     0.19522         0.94091\n",
      "  831     0.18051         0.93254\n",
      "  832     0.17729         0.92222\n",
      "  833     0.20165         0.90681\n",
      "  834     0.16132         0.94034\n",
      "  835     0.21784         0.91188\n",
      "  836     0.14321         0.95324\n",
      "  837     0.19118         0.93114\n",
      "  838     0.12632         0.95640\n",
      "  839     0.16634         0.94615\n",
      "  840     0.24644         0.92199\n",
      "  841     0.10822         0.97183\n",
      "  842     0.21127         0.93472\n",
      "  843     0.17045         0.94403\n",
      "  844     0.12979         0.95625\n",
      "  845     0.19518         0.94163\n",
      "  846     0.14098         0.95522\n",
      "  847     0.20575         0.92887\n",
      "  848     0.09796         0.97417\n",
      "  849     0.16808         0.92083\n",
      "  850     0.17039         0.94656\n",
      "  851     0.16068         0.94295\n",
      "  852     0.16580         0.94382\n",
      "  853     0.17020         0.93609\n",
      "  854     0.18204         0.92308\n",
      "  855     0.17674         0.94063\n",
      "  856     0.21054         0.94631\n",
      "  857     0.23163         0.94020\n",
      "  858     0.13486         0.94714\n",
      "  859     0.18000         0.94516\n",
      "  860     0.21472         0.93893\n",
      "  861     0.17157         0.94273\n",
      "  862     0.18403         0.93243\n",
      "  863     0.20865         0.92226\n",
      "  864     0.24460         0.91756\n",
      "  865     0.26370         0.93137\n",
      "  866     0.12382         0.96735\n",
      "  867     0.18045         0.94982\n",
      "  868     0.24793         0.91176\n",
      "  869     0.19842         0.93872\n",
      "  870     0.14386         0.94167\n",
      "  871     0.18889         0.93750\n",
      "  872     0.14498         0.95427\n",
      "  873     0.20953         0.92537\n",
      "  874     0.15193         0.95133\n",
      "  875     0.30990         0.91250\n",
      "  876     0.20328         0.93548\n",
      "  877     0.12432         0.94481\n",
      "  878     0.28803         0.91221\n",
      "  879     0.11798         0.96255\n",
      "  880     0.15543         0.94178\n",
      "  881     0.16016         0.95089\n",
      "  882     0.19763         0.92771\n",
      "  883     0.21101         0.93443\n",
      "  884     0.07535         0.97764\n",
      "  885     0.14867         0.94495\n",
      "  886     0.19911         0.95690\n",
      "  887     0.17824         0.93333\n",
      "  888     0.13373         0.94624\n",
      "  889     0.28302         0.91941\n",
      "  890     0.14902         0.93060\n",
      "  891     0.15511         0.95789\n",
      "  892     0.25729         0.91506\n",
      "  893     0.19073         0.93704\n",
      "  894     0.27597         0.90494\n",
      "  895     0.15492         0.94444\n",
      "  896     0.13855         0.95437\n",
      "  897     0.17933         0.94444\n",
      "  898     0.12042         0.96443\n",
      "  899     0.19934         0.93162\n",
      "  900     0.10368         0.95971\n",
      "  901     0.11599         0.97112\n",
      "  902     0.17778         0.94059\n",
      "  903     0.20790         0.91339\n",
      "  904     0.08415         0.96988\n",
      "  905     0.15643         0.95402\n",
      "  906     0.12873         0.95299\n",
      "  907     0.15138         0.94910\n",
      "  908     0.20252         0.93309\n",
      "  909     0.19846         0.93365\n",
      "  910     0.13917         0.95385\n",
      "  911     0.39339         0.88673\n",
      "  912     0.11649         0.96350\n",
      "  913     0.13994         0.95349\n",
      "  914     0.14515         0.94946\n",
      "  915     0.21295         0.93204\n",
      "  916     0.16177         0.94091\n",
      "  917     0.11519         0.95189\n",
      "  918     0.17864         0.93909\n",
      "  919     0.18012         0.95918\n",
      "  920     0.21873         0.94241\n",
      "  921     0.12101         0.95681\n",
      "  922     0.14404         0.96569\n",
      "  923     0.19955         0.93709\n",
      "  924     0.24906         0.90556\n",
      "  925     0.15977         0.93607\n",
      "  926     0.13249         0.95203\n",
      "  927     0.16424         0.93478\n",
      "  928     0.20502         0.92905\n",
      "  929     0.13321         0.96070\n",
      "  930     0.24476         0.91968\n",
      "  931     0.20941         0.93443\n",
      "  932     0.22545         0.92758\n",
      "  933     0.18280         0.92434\n",
      "  934     0.16939         0.93782\n",
      "  935     0.11528         0.95608\n",
      "  936     0.20188         0.93214\n",
      "  937     0.29478         0.88664\n",
      "  938     0.14180         0.95736\n",
      "  939     0.22458         0.91176\n",
      "  940     0.27001         0.90972\n",
      "  941     0.11452         0.96233\n",
      "  942     0.12613         0.95588\n",
      "  943     0.09029         0.97110\n",
      "  944     0.17219         0.94276\n",
      "  945     0.14632         0.95926\n",
      "  946     0.12377         0.95726\n",
      "  947     0.22946         0.90909\n",
      "  948     0.15608         0.94983\n",
      "  949     0.18138         0.95062\n",
      "  950     0.09666         0.96575\n",
      "  951     0.09933         0.96364\n",
      "  952     0.10643         0.96654\n",
      "  953     0.21456         0.93498\n",
      "  954     0.18113         0.95395\n",
      "  955     0.19745         0.92857\n",
      "  956     0.18833         0.94613\n",
      "  957     0.09512         0.96743\n",
      "  958     0.14363         0.93846\n",
      "  959     0.17035         0.94086\n",
      "  960     0.24598         0.90643\n",
      "  961     0.17570         0.92996\n",
      "  962     0.10773         0.95192\n",
      "  963     0.13759         0.95378\n",
      "  964     0.19154         0.93909\n",
      "  965     0.15562         0.94163\n",
      "  966     0.21767         0.91793\n",
      "  967     0.17950         0.93522\n",
      "  968     0.21428         0.90909\n",
      "  969     0.20127         0.92166\n",
      "  970     0.10275         0.95482\n",
      "  971     0.15122         0.96234\n",
      "  972     0.18621         0.94624\n",
      "  973     0.23538         0.92520\n",
      "  974     0.23448         0.93031\n",
      "  975     0.12988         0.96170\n",
      "  976     0.09386         0.97235\n",
      "  977     0.19605         0.95337\n",
      "  978     0.16542         0.93082\n",
      "  979     0.17816         0.92690\n",
      "  980     0.23424         0.90972\n",
      "  981     0.11859         0.96500\n",
      "  982     0.20307         0.95699\n",
      "  983     0.10722         0.96787\n",
      "  984     0.16422         0.94488\n",
      "  985     0.14535         0.94624\n",
      "  986     0.17452         0.93467\n",
      "  987     0.08199         0.97541\n",
      "  988     0.16298         0.94048\n",
      "  989     0.27930         0.91518\n",
      "  990     0.19848         0.95638\n",
      "  991     0.14269         0.95833\n",
      "  992     0.11884         0.94757\n",
      "  993     0.16100         0.94904\n",
      "  994     0.15058         0.93878\n",
      "  995     0.18008         0.95033\n",
      "  996     0.15774         0.93426\n",
      "  997     0.10674         0.96454\n",
      "  998     0.17228         0.95595\n",
      "  999     0.17703         0.95565\n",
      " 1000     0.12840         0.95222\n",
      " 1001     0.23470         0.90746\n",
      " 1002     0.16165         0.94909\n",
      " 1003     0.18997         0.92776\n",
      " 1004     0.19828         0.94352\n",
      " 1005     0.15119         0.93262\n",
      " 1006     0.42078         0.89286\n",
      " 1007     0.22791         0.91358\n",
      " 1008     0.18481         0.93317\n",
      " 1009     0.11920         0.96471\n",
      " 1010     0.25188         0.91684\n",
      " 1011     0.11173         0.95880\n",
      " 1012     0.16369         0.93359\n",
      " 1013     0.13646         0.95341\n",
      " 1014     0.06601         0.98842\n",
      " 1015     0.09840         0.96700\n",
      " 1016     0.15410         0.95238\n",
      " 1017     0.20605         0.92128\n",
      " 1018     0.12410         0.95166\n",
      " 1019     0.19666         0.93469\n",
      " 1020     0.11915         0.95058\n",
      " 1021     0.23728         0.92226\n",
      " 1022     0.17204         0.92075\n",
      " 1023     0.07947         0.97083\n",
      " 1024     0.18761         0.95327\n",
      " 1025     0.14673         0.93657\n",
      " 1026     0.12997         0.94937\n",
      " 1027     0.14660         0.94510\n",
      " 1028     0.15140         0.93774\n",
      " 1029     0.10669         0.96654\n",
      " 1030     0.07650         0.98187\n",
      " 1031     0.23550         0.92537\n",
      " 1032     0.13153         0.96099\n",
      " 1033     0.20823         0.92437\n",
      " 1034     0.23585         0.91497\n",
      " 1035     0.19195         0.92647\n",
      " 1036     0.22747         0.91358\n",
      " 1037     0.22368         0.91455\n",
      " 1038     0.19458         0.93878\n",
      " 1039     0.09766         0.96491\n",
      " 1040     0.11575         0.96970\n",
      " 1041     0.20313         0.93103\n",
      " 1042     0.16960         0.93173\n",
      " 1043     0.21761         0.92857\n",
      " 1044     0.12315         0.95639\n",
      " 1045     0.10781         0.96646\n",
      " 1046     0.24259         0.92537\n",
      " 1047     0.14609         0.94643\n",
      " 1048     0.11242         0.96169\n",
      " 1049     0.14821         0.95349\n",
      " 1050     0.16472         0.93272\n",
      " 1051     0.15993         0.95639\n",
      " 1052     0.25284         0.90580\n",
      " 1053     0.15333         0.94834\n",
      " 1054     0.12116         0.95486\n",
      " 1055     0.12832         0.97034\n",
      " 1056     0.22671         0.89965\n",
      " 1057     0.16840         0.93182\n",
      " 1058     0.25105         0.91329\n",
      " 1059     0.10026         0.97436\n",
      " 1060     0.29751         0.93909\n",
      " 1061     0.10640         0.97106\n",
      " 1062     0.21456         0.91234\n",
      " 1063     0.11226         0.95820\n",
      " 1064     0.11578         0.96691\n",
      " 1065     0.11709         0.95305\n",
      " 1066     0.14774         0.93750\n",
      " 1067     0.09679         0.97318\n",
      " 1068     0.15174         0.94663\n",
      " 1069     0.24491         0.92265\n"
     ]
    }
   ],
   "source": [
    "# Get the metrics dataframe\n",
    "metrics_df = get_finetuning_metrics(client, job_id)\n",
    "\n",
    "# Display only train_loss and train_accuracy columns\n",
    "print(\"Fine-tuning metrics for job:\", job_id)\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Check if the columns exist (column names might vary)\n",
    "if 'train_loss' in metrics_df.columns and 'train_accuracy' in metrics_df.columns:\n",
    "    # Print only these two columns with step for context\n",
    "    print(metrics_df[['step', 'train_loss', 'train_accuracy']].to_string(index=False))\n",
    "elif 'training_loss' in metrics_df.columns and 'training_accuracy' in metrics_df.columns:\n",
    "    # Alternative column names\n",
    "    print(metrics_df[['step', 'training_loss', 'training_accuracy']].to_string(index=False))\n",
    "else:\n",
    "    # If column names are different, print available columns\n",
    "    print(\"Columns 'train_loss' and 'train_accuracy' not found.\")\n",
    "    print(\"Available columns:\", metrics_df.columns.tolist())\n",
    "    \n",
    "    # Try to find columns with similar names\n",
    "    loss_cols = [col for col in metrics_df.columns if 'loss' in col.lower()]\n",
    "    accuracy_cols = [col for col in metrics_df.columns if 'accuracy' in col.lower() or 'acc' in col.lower()]\n",
    "    \n",
    "    if loss_cols and accuracy_cols:\n",
    "        print(f\"Using columns: {loss_cols[0]} and {accuracy_cols[0]}\")\n",
    "        print(metrics_df[['step', loss_cols[0], accuracy_cols[0]]].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6d437b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydata-book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
