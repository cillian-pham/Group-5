{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d747364-6d2c-401e-a51f-0389fea44f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (1.68.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: numpy>=2.0.2 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from openai) (2.0.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from openai) (2.10.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: sounddevice>=0.5.1 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from openai) (0.5.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from openai) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from sounddevice>=0.5.1->openai) (1.17.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.5.1->openai) (2.22)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cillian\\miniconda3\\envs\\pydata-book\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0aa844fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "import time\n",
    "import tiktoken\n",
    "from collections import defaultdict, Counter\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Dict, Tuple, Set\n",
    "from tqdm import tqdm  # For progress tracking\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fbb35d-a2cc-4563-9ddd-64ac3e647899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAIN DATA:\n",
      "                                                                                                                                                                                                                                text  \\\n",
      "0              NEW YORK (Reuters) - Apple Inc Chief Executive Steve Jobs sought to soothe investor concerns about his health on Monday, saying his weight loss was caused by a hormone imbalance that is relatively simple to treat.   \n",
      "1                                                          Last week, Citigroup Inc's ( C.N ) Chief Executive Vikram Pandit said that he, Chairman Win Bischoff, and senior adviser Robert Rubin would not receive bonuses for 2008.   \n",
      "2                                                             Lehman Brothers LEH.N shares fell sharply on Monday on speculation that the investment bank could be bought for $15 a share, a price well below current market levels.   \n",
      "3  Franz told Reuters that Fiat Chief Executive Sergio Marchionne had said at a meeting on Monday he foresaw closing Opel's Kaiserslautern engine plant in Germany and other Fiat and Opel manufacturing sites in England and Italy.   \n",
      "4                                                                  \"In an industry that has a poor track record for M&A execution, Fiat CEO Sergio Marchionne has his work cut out for him,\" said Morgan Stanley analyst Adam Jonas.   \n",
      "\n",
      "                                                                                                   annotations  \n",
      "0                      [Apple Inc ; Steve Jobs ; founded_by, Apple Inc ; Steve Jobs ; chief_executive_officer]  \n",
      "1                                                                       [Vikram Pandit ; Citigroup ; employer]  \n",
      "2  [Lehman Brothers ; investment bank ; product_or_material_produced, Lehman Brothers ; investment ; industry]  \n",
      "3                                                                        [Sergio Marchionne ; Fiat ; employer]  \n",
      "4                                                                        [Sergio Marchionne ; Fiat ; employer]  \n",
      "\n",
      "DEV DATA:\n",
      "                                                                                                                                                                                                                        text  \\\n",
      "0                                                                                                 Yum China will become a franchise of Yum Brands in Mainland China, the parent of KFC, Pizza Hut and Taco Bell chains said.   \n",
      "1                                                            Warren Buffett's Berkshire Hathaway (Sao Paolo: BERK34F.SA - news ) this month also launched its first cyber policies through its specialty insurance division.   \n",
      "2  In the wake of last year's attack on Sony Pictures Entertainment, parent Sony Corp said its financial condition could suffer if it were attacked again, since current policies \"might not cover all expenses and losses.\"   \n",
      "3                                                 Oct 12 Investment bank Cantor Fitzgerald Europe, part of Cantor Fitzgerald, has hired four executives from financial services firm Shore Capital for its Edinburgh office.   \n",
      "4                                     Collaborating with University of Connecticut doctoral student Devon Goss, Matthew Hughey researched nearly 24,000 English-language newspaper articles across the globe from 2003-2014.   \n",
      "\n",
      "                                                                                                                                                                                                      annotations  \n",
      "0  [Yum China ; Pizza Hut ; brand, Yum China ; Taco Bell ; brand, Yum Brands ; Taco Bell ; subsidiary, Yum Brands ; Taco Bell ; owner_of, Yum Brands ; Pizza Hut ; subsidiary, Yum Brands ; Pizza Hut ; owner_of]  \n",
      "1                                                                                                                                                                     [Berkshire Hathaway ; insurance ; industry]  \n",
      "2                            [Sony Pictures ; Sony ; owned_by, Sony ; Sony Pictures ; owner_of, Sony Pictures ; Sony ; founded_by, Sony Pictures ; Sony ; parent_organization, Sony ; Sony Pictures ; subsidiary]  \n",
      "3                                                                                      [Cantor Fitzgerald ; financial services ; industry, Cantor Fitzgerald ; financial services ; product_or_material_produced]  \n",
      "4                                                                                                                                                         [Matthew Hughey ; University of Connecticut ; employer]  \n",
      "\n",
      "TEST DATA:\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 text  \\\n",
      "0  Wednesday, July 8, 2015 10:30AM IST (5:00AM GMT) Rimini Street Comment on Oracle Litigation Las Vegas, United States Rimini Street, Inc., the leading independent provider of enterprise software support for SAP AG’s (NYSE:SAP) Business Suite and BusinessObjects software and Oracle Corporation’s (NYSE:ORCL) Siebel , PeopleSoft , JD Edwards , E-Business Suite , Oracle Database , Hyperion and Oracle Retail software, today issued a statement on the Oracle litigation.   \n",
      "1                                                                                                                                                                                                                                                                                      The Daily Show with Trevor Noah premieres tonight... and while the show will be based on Comedy Central, Viacom pans to simulcast the debut across all of its networks, including VH1 and MTV.   \n",
      "2                                                                                                                                                                                                                                                                                                                               \"Our results for the quarter show very balanced performance across our business lines,\" said Citigroup chief executive Michael Corbat in a statement.   \n",
      "3                                                                                                                                                                                                                                          Saudi Arabian budget carrier flynas, which made its first profit this year, is in talks with plane manufacturers Airbus and Boeing as it seeks to purchase four new aircraft over the next four years, its chief executive told reporters.   \n",
      "4                                                                                                                                                                                                                                                                                                                                                        First Eagle is currently owned by members of the Arnhold family, private equity firm TA Associates Management and employees.   \n",
      "\n",
      "                                          annotations  \n",
      "0              [PeopleSoft ; JD Edwards ; subsidiary]  \n",
      "1                           [VH1 ; Viacom ; owned_by]  \n",
      "2             [Michael Corbat ; Citigroup ; employer]  \n",
      "3  [Airbus ; aircraft ; product_or_material_produced]  \n",
      "4         [TA Associates ; private equity ; industry]  \n"
     ]
    }
   ],
   "source": [
    "#1.Process Data\n",
    "\n",
    "# function to extract sentences and annotations\n",
    "def extract_data(text):\n",
    "    data = []\n",
    "    sentences = text.strip().split('\\n')\n",
    "    for sentence in sentences:\n",
    "        parts = sentence.split('|')\n",
    "        if len(parts) < 2:  # Ensure valid format\n",
    "            continue  # Skip invalid lines\n",
    "        text = parts[0].strip()\n",
    "        annotations = [part.strip() for part in parts[1:]]\n",
    "        data.append({'text': text, 'annotations': annotations})\n",
    "    return data\n",
    "\n",
    "# function to read and process a file\n",
    "def process_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        return extract_data(text)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: File {file_path} not found.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Process train.txt\n",
    "train_data = process_file('train.txt')\n",
    "df_train = pd.DataFrame(train_data) if train_data else pd.DataFrame()\n",
    "if not df_train.empty:\n",
    "    print(\"\\nTRAIN DATA:\")\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    print(df_train.head())\n",
    "else:\n",
    "    print(\"No data extracted from train.txt\")\n",
    "\n",
    "# Process dev.txt\n",
    "dev_data = process_file('dev.txt')\n",
    "df_dev = pd.DataFrame(dev_data) if dev_data else pd.DataFrame()\n",
    "if not df_dev.empty:\n",
    "    print(\"\\nDEV DATA:\")\n",
    "    print(df_dev.head())\n",
    "else:\n",
    "    print(\"No data extracted from dev.txt\")\n",
    "\n",
    "# Process test.txt\n",
    "test_data = process_file('test.txt')\n",
    "df_test = pd.DataFrame(test_data) if test_data else pd.DataFrame()\n",
    "if not df_test.empty:\n",
    "    print(\"\\nTEST DATA:\")\n",
    "    print(df_test.head())\n",
    "else:\n",
    "    print(\"No data extracted from test.txt\")\n",
    "\n",
    "# Now you have three separate DataFrames:\n",
    "# df_train - containing training data\n",
    "# df_dev - containing development/validation data\n",
    "# df_test - containing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c478aff-c2ae-4659-be25-b97368c64ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text           New 'Bratz' more Taylor Swift than Britney: CEO Ashley Holt, CNBC CNBC.com SHARES Bratz makeover no child's play: CEO Isaac Larian, MGA Entertainment CEO, discusses the relaunch of the Bratz dolls and its battle to take on Mattel's Barbie.\n",
      "annotations                                                                                                                                                                                                              [Bratz ; MGA Entertainment ; creator]\n",
      "Name: 2069, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import html\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text by:\n",
    "    1. Decoding HTML entities (like &#039; &#959; etc.)\n",
    "    2. Removing HTML tags (like <p> </p>)\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    # Decode HTML entities (converts &#039; to ', &#959; to ο, etc.)\n",
    "    text = html.unescape(text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "    # Replace Unicode right single quotation mark with a standard single quote\n",
    "    text = text.replace('\\u2019', \"'\")\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply cleaning to the 'text' column in all three dataframes\n",
    "for df in [df_train, df_dev, df_test]:\n",
    "    if not df.empty and 'text' in df.columns:\n",
    "        df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "print(df_train.iloc[2069])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16e412e0-07dc-4a62-a824-2609af38c294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text           Oil and gas explorer Roxi Petroleum (Other OTC: ROXIF - news ) has reached an agreement to cancel royalty payments from its flagship asset.\n",
      "annotations                                                                 [ROXI ; London ; location_of_formation, ROXI ; London ; headquarters_location]\n",
      "Name: 2119, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_source_info(text):\n",
    "    \"\"\"\n",
    "    Clean source information from the beginning of text.\n",
    "    Looks for specific patterns in the first 50 characters and removes everything\n",
    "    from the beginning to the end of the matched pattern.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    # Check only the first 50 characters (or all if less than 50)\n",
    "    first_part = text[:50] if len(text) > 50 else text\n",
    "    \n",
    "    # Patterns to look for in the beginning\n",
    "    patterns = [\n",
    "        r'\\(AP\\) -+ ',\n",
    "        r'\\(AP\\) _ ',\n",
    "        r'\\(AP\\) — ',\n",
    "        r'\\(IANS\\) ',\n",
    "        r'\\(Reuters\\) - ',\n",
    "        r'\\(TheStreet\\) -- ',\n",
    "        r'\\(GLOBE NEWSWIRE\\) -- ',\n",
    "        r'\\(ShareCast\\) - \\(ShareCast News\\) - ',\n",
    "        r'CNBC CNBC\\.com SHARES ',\n",
    "        r'BST - '\n",
    "    ]\n",
    "    \n",
    "    # Find the earliest match of any pattern\n",
    "    earliest_match = None\n",
    "    earliest_end = len(text)\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, first_part)\n",
    "        if match and match.end() < earliest_end:\n",
    "            earliest_match = match\n",
    "            earliest_end = match.end()\n",
    "    \n",
    "    # If a match was found, remove everything up to the end of the match\n",
    "    if earliest_match:\n",
    "        return text[earliest_end:].strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_source_info_2(text):\n",
    "    \"\"\"\n",
    "    Cleans a text string based on the presence of \" IST \" and \"|\"\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ist_index = text.find(\" IST \")\n",
    "        if ist_index != -1:\n",
    "            pipe_index = text.rfind(\"|\", 0, ist_index)  # Search backward for \"|\"\n",
    "\n",
    "            if pipe_index != -1:\n",
    "                return text[:pipe_index].strip() + text[ist_index + len(\" IST \"):].strip()\n",
    "            else:\n",
    "                return text[ist_index + len(\" IST \"):].strip()\n",
    "        return text  # Return original text if \" IST \" is not found\n",
    "    except AttributeError:\n",
    "        return text  # Handle cases where the input might not be a string\n",
    "    \n",
    "# Apply cleaning to the 'text' column in all three dataframes\n",
    "for df in [df_train, df_dev, df_test]:\n",
    "    if not df.empty and 'text' in df.columns:\n",
    "        df['text'] = df['text'].apply(clean_source_info)\n",
    "\n",
    "for df in [df_train, df_dev, df_test]:\n",
    "    if not df.empty and 'text' in df.columns:\n",
    "        df['text'] = df['text'].apply(clean_source_info_2)\n",
    "\n",
    "\n",
    "print(df_train.iloc[2119])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c273d31f-2642-4087-a302-afa61874b68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text           Chemicals group Elementis saw its first half pre-tax profit decline, as its oil and gas related and businesses reported a drop in sales.\n",
      "annotations                                                                                                [Elementis ; London ; headquarters_location]\n",
      "Name: 2127, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_parenthetical_expressions(text):\n",
    "    \"\"\"\n",
    "    Remove all text within parentheses () and square brackets []\n",
    "    from the given text.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    # Remove text within parentheses ()\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    \n",
    "    # Remove text within square brackets []\n",
    "    text = re.sub(r'\\[[^\\]]*\\]', '', text)\n",
    "    \n",
    "    # Clean up any double spaces created by the removals\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply cleaning to the 'text' column in all three dataframes\n",
    "for df in [df_train, df_dev, df_test]:\n",
    "    if not df.empty and 'text' in df.columns:\n",
    "        df['text'] = df['text'].apply(remove_parenthetical_expressions)\n",
    "\n",
    "print(df_train.iloc[2127])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3f9c755-c80d-4df8-963b-9a93a5794439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text           National Bank of Belgium Governor Luc Coene speaks at the Deloitte Global Financial Industry Partner Meeting - 1030 GMT.\n",
      "annotations                                                                                      [Luc Coene ; governor ; position_held]\n",
      "Name: 4183, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_location_dash(text):\n",
    "    \"\"\"\n",
    "    Clean location information from the beginning of text.\n",
    "    Looks for \"WORD - \" pattern in the first 50 characters and \n",
    "    removes everything from the beginning to the end of the matched pattern.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    # Check only the first 50 characters (or all if less than 50)\n",
    "    first_part = text[:50] if len(text) > 50 else text\n",
    "    \n",
    "    # Pattern to look for location followed by dash: word(s) followed by \" - \"\n",
    "    # This will match patterns like \"DUBLIN - \", \"NEW YORK - \", etc.\n",
    "    pattern = r'^\\w+(\\s+\\w+)* - '\n",
    "    \n",
    "    match = re.search(pattern, first_part)\n",
    "    if match:\n",
    "        return text[match.end():].strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply cleaning to the 'text' column in all three dataframes\n",
    "for df in [df_train, df_dev, df_test]:\n",
    "    if not df.empty and 'text' in df.columns:\n",
    "        df['text'] = df['text'].apply(clean_location_dash)\n",
    "\n",
    "print(df_train.iloc[4183])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8aa78250-2622-4d2a-a600-dfefa9c62c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'product or material produced', 'manufacturer', 'distributed by', 'industry', 'position held', 'original broadcaster', 'owned by', 'founded by', 'distribution format', 'headquarters location', 'stock exchange', 'currency', 'parent organization', 'chief executive officer', 'director or manager', 'owner of', 'operator', 'member of', 'employer', 'chairperson', 'platform', 'subsidiary', 'legal form', 'publisher', 'developer', 'brand', 'business division', 'location of formation', 'creator'\n"
     ]
    }
   ],
   "source": [
    "# Load and update relations from the file\n",
    "with open('relations.txt', 'r') as file:\n",
    "    relations = [line.replace(\"product/material produced\", \"product or material produced\")\n",
    "                     .replace(\"director/manager\", \"director or manager\") \n",
    "                 for line in file.read().splitlines()]\n",
    "\n",
    "\n",
    "# Convert to formatted string\n",
    "relations_list = ', '.join(f\"'{relation}'\" for relation in relations)\n",
    " \n",
    "print(relations_list)  # Check the updated relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41f0623c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple Inc Chief Executive Steve Jobs sought to soothe investor concerns about his health on Monday, saying his weight loss was caused by a hormone imbalance that is relatively simple to treat.</td>\n",
       "      <td>[Apple Inc ; Steve Jobs ; founded by, Apple Inc ; Steve Jobs ; chief executive officer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Last week, Citigroup Inc's Chief Executive Vikram Pandit said that he, Chairman Win Bischoff, and senior adviser Robert Rubin would not receive bonuses for 2008.</td>\n",
       "      <td>[Vikram Pandit ; Citigroup ; employer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lehman Brothers LEH.N shares fell sharply on Monday on speculation that the investment bank could be bought for $15 a share, a price well below current market levels.</td>\n",
       "      <td>[Lehman Brothers ; investment bank ; product or material produced, Lehman Brothers ; investment ; industry]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Franz told Reuters that Fiat Chief Executive Sergio Marchionne had said at a meeting on Monday he foresaw closing Opel's Kaiserslautern engine plant in Germany and other Fiat and Opel manufacturing sites in England and Italy.</td>\n",
       "      <td>[Sergio Marchionne ; Fiat ; employer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"In an industry that has a poor track record for M&amp;A execution, Fiat CEO Sergio Marchionne has his work cut out for him,\" said Morgan Stanley analyst Adam Jonas.</td>\n",
       "      <td>[Sergio Marchionne ; Fiat ; employer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5695</th>\n",
       "      <td>In particular, he said leases of used A330-200 aircraft from Airbus Group SE would be five times cheaper than new ones.</td>\n",
       "      <td>[Airbus ; aircraft ; product or material produced]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5696</th>\n",
       "      <td>The company is an omnipresent in households the world over thanks to its operations across multiple consumer goods markets -- from Nurofen pain suppressants and Dettol disinfectant through to French's mustard, Reckitt Benckiser has its fingers in many pies.</td>\n",
       "      <td>[Dettol ; Reckitt ; owned by]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5697</th>\n",
       "      <td>Related articles Mark Carney is Governor of the Bank of England</td>\n",
       "      <td>[Mark Carney ; Governor of the Bank of England ; position held]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5698</th>\n",
       "      <td>E-commerce company eBay Inc is expected to report third-quarter revenue below expectations, according to some analysts.</td>\n",
       "      <td>[eBay ; e-commerce ; industry]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5699</th>\n",
       "      <td>Hard disk drive maker Western Digital Corp is in advanced talks to acquire SanDisk, people familiar with the matter told Reuters on Monday.</td>\n",
       "      <td>[Western Digital ; hard disk drive ; product or material produced]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5700 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                   text  \\\n",
       "0                                                                      Apple Inc Chief Executive Steve Jobs sought to soothe investor concerns about his health on Monday, saying his weight loss was caused by a hormone imbalance that is relatively simple to treat.   \n",
       "1                                                                                                     Last week, Citigroup Inc's Chief Executive Vikram Pandit said that he, Chairman Win Bischoff, and senior adviser Robert Rubin would not receive bonuses for 2008.   \n",
       "2                                                                                                Lehman Brothers LEH.N shares fell sharply on Monday on speculation that the investment bank could be bought for $15 a share, a price well below current market levels.   \n",
       "3                                     Franz told Reuters that Fiat Chief Executive Sergio Marchionne had said at a meeting on Monday he foresaw closing Opel's Kaiserslautern engine plant in Germany and other Fiat and Opel manufacturing sites in England and Italy.   \n",
       "4                                                                                                     \"In an industry that has a poor track record for M&A execution, Fiat CEO Sergio Marchionne has his work cut out for him,\" said Morgan Stanley analyst Adam Jonas.   \n",
       "...                                                                                                                                                                                                                                                                 ...   \n",
       "5695                                                                                                                                            In particular, he said leases of used A330-200 aircraft from Airbus Group SE would be five times cheaper than new ones.   \n",
       "5696  The company is an omnipresent in households the world over thanks to its operations across multiple consumer goods markets -- from Nurofen pain suppressants and Dettol disinfectant through to French's mustard, Reckitt Benckiser has its fingers in many pies.   \n",
       "5697                                                                                                                                                                                                    Related articles Mark Carney is Governor of the Bank of England   \n",
       "5698                                                                                                                                            E-commerce company eBay Inc is expected to report third-quarter revenue below expectations, according to some analysts.   \n",
       "5699                                                                                                                        Hard disk drive maker Western Digital Corp is in advanced talks to acquire SanDisk, people familiar with the matter told Reuters on Monday.   \n",
       "\n",
       "                                                                                                      annotations  \n",
       "0                         [Apple Inc ; Steve Jobs ; founded by, Apple Inc ; Steve Jobs ; chief executive officer]  \n",
       "1                                                                          [Vikram Pandit ; Citigroup ; employer]  \n",
       "2     [Lehman Brothers ; investment bank ; product or material produced, Lehman Brothers ; investment ; industry]  \n",
       "3                                                                           [Sergio Marchionne ; Fiat ; employer]  \n",
       "4                                                                           [Sergio Marchionne ; Fiat ; employer]  \n",
       "...                                                                                                           ...  \n",
       "5695                                                           [Airbus ; aircraft ; product or material produced]  \n",
       "5696                                                                                [Dettol ; Reckitt ; owned by]  \n",
       "5697                                              [Mark Carney ; Governor of the Bank of England ; position held]  \n",
       "5698                                                                               [eBay ; e-commerce ; industry]  \n",
       "5699                                           [Western Digital ; hard disk drive ; product or material produced]  \n",
       "\n",
       "[5700 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_annotations(annotations):  \n",
    "    return [annotation.replace('_', ' ') for annotation in annotations]\n",
    "\n",
    "# Define the function to parse annotations\n",
    "def parse_annotation(annotation_str):\n",
    "    \"\"\"\n",
    "    Parse an annotation string in the format \"entity1 ; entity2 ; relation\"\n",
    "    Returns a tuple (entity1, entity2, relation) or None if parsing fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parts = annotation_str.split(\" ; \")\n",
    "        if len(parts) == 3:\n",
    "            entity1, entity2, relation = parts\n",
    "            return (entity1, entity2, relation)\n",
    "        return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Define the directional relationship pairs\n",
    "relationship_pairs = {\n",
    "    \"product or material produced\": \"manufacturer\",\n",
    "    \"manufacturer\": \"product or material produced\",\n",
    "    \"position held\": \"chief executive officer\",\n",
    "    \"position held\": \"director or manager\",\n",
    "    \"position held\": \"chairperson\",\n",
    "    \"chief executive officer\": \"position held\",\n",
    "    \"director or manager\": \"position held\",\n",
    "    \"chairperson\": \"position held\",\n",
    "    \"owned by\": \"owner of\",\n",
    "    \"owner of\": \"owned by\",\n",
    "    \"parent organization\": \"subsidiary\",\n",
    "    \"subsidiary\": \"parent organization\",\n",
    "    \"creator\": \"founded by\",\n",
    "    \"founded by\": \"creator\"\n",
    "}\n",
    "\n",
    "def process_annotations(annotations):\n",
    "    \"\"\"\n",
    "    Process annotations in the given list of annotations.\n",
    "    This function identifies existing relationships and adds missing reverse relationships.\n",
    "    \"\"\"\n",
    "    existing_relations = set()\n",
    "\n",
    "    # Extract existing relationships\n",
    "    for annotation in annotations:\n",
    "        # Replace \"director_/_manager\" with \"director_or_manager\" before parsing\n",
    "        annotation = annotation.replace(\"director_/_manager\", \"director or manager\")\n",
    "\n",
    "        parsed = parse_annotation(annotation)\n",
    "        if parsed:\n",
    "            existing_relations.add(parsed)\n",
    "\n",
    "    # Identify missing reverse relationships\n",
    "    new_relations = set()\n",
    "    for entity1, entity2, relation in existing_relations:\n",
    "        if relation in relationship_pairs:\n",
    "            reverse_relation = relationship_pairs[relation]\n",
    "            if (entity2, entity1, reverse_relation) not in existing_relations:\n",
    "                new_relations.add((entity2, entity1, reverse_relation))\n",
    "        elif relation in relationship_pairs.values():\n",
    "            # Get all keys that map to this value\n",
    "            reverse_relations = [k for k, v in relationship_pairs.items() if v == relation]\n",
    "            if reverse_relations:\n",
    "                reverse_relation = reverse_relations[0]  # Take the first one\n",
    "                if (entity2, entity1, reverse_relation) not in existing_relations:\n",
    "                    new_relations.add((entity2, entity1, reverse_relation))\n",
    "\n",
    "    # Return all relations combined as a list of formatted strings\n",
    "    return list(annotations) + [\" ; \".join(relation) for relation in new_relations]\n",
    "\n",
    "# Process each DataFrame\n",
    "for df in [df_train, df_dev, df_test]:\n",
    "    if not df.empty and 'annotations' in df.columns:\n",
    "        df['annotations'] = df['annotations'].apply(process_annotations)\n",
    "        df['annotations'] = df['annotations'].apply(clean_annotations)\n",
    "\n",
    "# Display the updated DataFrames\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10fccfda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple Inc Chief Executive Steve Jobs sought to soothe investor concerns about his health on Monday, saying his weight loss was caused by a hormone imbalance that is relatively simple to treat.</td>\n",
       "      <td>{(Apple Inc, Steve Jobs, founded by), (Apple Inc, Steve Jobs, chief executive officer)}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Last week, Citigroup Inc's Chief Executive Vikram Pandit said that he, Chairman Win Bischoff, and senior adviser Robert Rubin would not receive bonuses for 2008.</td>\n",
       "      <td>{(Vikram Pandit, Citigroup, employer)}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lehman Brothers LEH.N shares fell sharply on Monday on speculation that the investment bank could be bought for $15 a share, a price well below current market levels.</td>\n",
       "      <td>{(Lehman Brothers, investment, industry), (Lehman Brothers, investment bank, product or material produced)}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Franz told Reuters that Fiat Chief Executive Sergio Marchionne had said at a meeting on Monday he foresaw closing Opel's Kaiserslautern engine plant in Germany and other Fiat and Opel manufacturing sites in England and Italy.</td>\n",
       "      <td>{(Sergio Marchionne, Fiat, employer)}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"In an industry that has a poor track record for M&amp;A execution, Fiat CEO Sergio Marchionne has his work cut out for him,\" said Morgan Stanley analyst Adam Jonas.</td>\n",
       "      <td>{(Sergio Marchionne, Fiat, employer)}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5695</th>\n",
       "      <td>In particular, he said leases of used A330-200 aircraft from Airbus Group SE would be five times cheaper than new ones.</td>\n",
       "      <td>{(Airbus, aircraft, product or material produced)}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5696</th>\n",
       "      <td>The company is an omnipresent in households the world over thanks to its operations across multiple consumer goods markets -- from Nurofen pain suppressants and Dettol disinfectant through to French's mustard, Reckitt Benckiser has its fingers in many pies.</td>\n",
       "      <td>{(Dettol, Reckitt, owned by)}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5697</th>\n",
       "      <td>Related articles Mark Carney is Governor of the Bank of England</td>\n",
       "      <td>{(Mark Carney, Governor of the Bank of England, position held)}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5698</th>\n",
       "      <td>E-commerce company eBay Inc is expected to report third-quarter revenue below expectations, according to some analysts.</td>\n",
       "      <td>{(eBay, e-commerce, industry)}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5699</th>\n",
       "      <td>Hard disk drive maker Western Digital Corp is in advanced talks to acquire SanDisk, people familiar with the matter told Reuters on Monday.</td>\n",
       "      <td>{(Western Digital, hard disk drive, product or material produced)}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5700 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                   text  \\\n",
       "0                                                                      Apple Inc Chief Executive Steve Jobs sought to soothe investor concerns about his health on Monday, saying his weight loss was caused by a hormone imbalance that is relatively simple to treat.   \n",
       "1                                                                                                     Last week, Citigroup Inc's Chief Executive Vikram Pandit said that he, Chairman Win Bischoff, and senior adviser Robert Rubin would not receive bonuses for 2008.   \n",
       "2                                                                                                Lehman Brothers LEH.N shares fell sharply on Monday on speculation that the investment bank could be bought for $15 a share, a price well below current market levels.   \n",
       "3                                     Franz told Reuters that Fiat Chief Executive Sergio Marchionne had said at a meeting on Monday he foresaw closing Opel's Kaiserslautern engine plant in Germany and other Fiat and Opel manufacturing sites in England and Italy.   \n",
       "4                                                                                                     \"In an industry that has a poor track record for M&A execution, Fiat CEO Sergio Marchionne has his work cut out for him,\" said Morgan Stanley analyst Adam Jonas.   \n",
       "...                                                                                                                                                                                                                                                                 ...   \n",
       "5695                                                                                                                                            In particular, he said leases of used A330-200 aircraft from Airbus Group SE would be five times cheaper than new ones.   \n",
       "5696  The company is an omnipresent in households the world over thanks to its operations across multiple consumer goods markets -- from Nurofen pain suppressants and Dettol disinfectant through to French's mustard, Reckitt Benckiser has its fingers in many pies.   \n",
       "5697                                                                                                                                                                                                    Related articles Mark Carney is Governor of the Bank of England   \n",
       "5698                                                                                                                                            E-commerce company eBay Inc is expected to report third-quarter revenue below expectations, according to some analysts.   \n",
       "5699                                                                                                                        Hard disk drive maker Western Digital Corp is in advanced talks to acquire SanDisk, people familiar with the matter told Reuters on Monday.   \n",
       "\n",
       "                                                                                                      annotations  \n",
       "0                         {(Apple Inc, Steve Jobs, founded by), (Apple Inc, Steve Jobs, chief executive officer)}  \n",
       "1                                                                          {(Vikram Pandit, Citigroup, employer)}  \n",
       "2     {(Lehman Brothers, investment, industry), (Lehman Brothers, investment bank, product or material produced)}  \n",
       "3                                                                           {(Sergio Marchionne, Fiat, employer)}  \n",
       "4                                                                           {(Sergio Marchionne, Fiat, employer)}  \n",
       "...                                                                                                           ...  \n",
       "5695                                                           {(Airbus, aircraft, product or material produced)}  \n",
       "5696                                                                                {(Dettol, Reckitt, owned by)}  \n",
       "5697                                              {(Mark Carney, Governor of the Bank of England, position held)}  \n",
       "5698                                                                               {(eBay, e-commerce, industry)}  \n",
       "5699                                           {(Western Digital, hard disk drive, product or material produced)}  \n",
       "\n",
       "[5700 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to process annotations into a set of triplets\n",
    "def convert_to_triplet_set(annotation):\n",
    "    if isinstance(annotation, str):  # If stored as a string\n",
    "        annotation = annotation.strip(\"[]\")  # Remove brackets if mistakenly included\n",
    "        annotation = annotation.split(\",\")  # Split into list\n",
    " \n",
    "    if isinstance(annotation, list):  # Ensure it's a list now\n",
    "        triplets = set()\n",
    "        for item in annotation:\n",
    "            parts = [x.strip() for x in item.split(\";\")]  # Split on ';' and remove extra spaces\n",
    "            if len(parts) == 3:  # Only take valid triplets\n",
    "                triplets.add(tuple(parts))\n",
    "        return triplets\n",
    "    return annotation  # Return original if not processable\n",
    " \n",
    "# Apply transformation\n",
    "\n",
    "for df in [df_train, df_dev, df_test]:\n",
    "    df[\"annotations\"] = df[\"annotations\"].apply(convert_to_triplet_set)\n",
    "\n",
    "df_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "763d7b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are an expert in relationship extraction. Your task is to extract relationships between entities in the given text.\n",
    "\n",
    "## Output Format\n",
    "Return a JSON object:\n",
    "```json\n",
    "{{\n",
    "  \"relations\": [\n",
    "    [\"Entity1\", \"Entity2\", \"relation_type\"],\n",
    "    ...\n",
    "  ]\n",
    "}}\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Steps to follow :\n",
    "\n",
    "Steps for Extraction\n",
    "1. Read the entire sentence and understand the contextual meaning of the entire sentence.\n",
    "2. Identify Entities\n",
    "\n",
    "•⁠  Extract all relevant entities such as\n",
    "- ORG: Companies, corporations, agencies (Apple, Goldman Sachs, NASA)\n",
    "- PERSON: Individual humans (Tim Cook, Janet Yellen)\n",
    "- PRODUCT: Goods, services, software (iPhone, Windows 11)\n",
    "- LOCATION: Geographic places (New York, Japan)\n",
    "- FINANCE: Currencies, exchanges (USD, NASDAQ)\n",
    "- TITLE: Job positions (CEO, Director)\n",
    "- BRAND: Consumer identities (Lexus, Instagram)\n",
    "- PLATFORM: Digital ecosystems (iOS, AWS)\n",
    "\n",
    "•⁠  Ignore entities without a direct connection to another entity.\n",
    "\n",
    "3. Pair Entities Only If a Strong Connection Exists based on (ins_dic)\n",
    "Do not generate all possible entity pairs.\n",
    "- ORG-ORG:\n",
    "- owned by\n",
    "- parent organization\n",
    "- owner of\n",
    "- subsidiary\n",
    "- business division\n",
    "- platform\n",
    "- operator\n",
    "- brand\n",
    "- stock exchange\n",
    "- legal form\n",
    "\n",
    "ORG-PERSON:\n",
    "- owned by\n",
    "- employer\n",
    "- member of\n",
    "- founded by\n",
    "\n",
    "PERSON-ORG:\n",
    "- creator\n",
    "- member of\n",
    "- chief executive officer\n",
    "- director or manager\n",
    "- chairperson\n",
    "- owner of\n",
    "\n",
    "ORG-LOCATION:\n",
    "- headquarters location\n",
    "- location of formation\n",
    "\n",
    "ORG-PRODUCT:\n",
    "- manufacturer\n",
    "- distributed by\n",
    "- developer\n",
    "- industry\n",
    "\n",
    "PRODUCT-ORG:\n",
    "- original broadcaster\n",
    "- publisher\n",
    "- product or material produced\n",
    "\n",
    "PERSON-PRODUCT:\n",
    "- developer\n",
    "- manufacturer\n",
    "\n",
    "PERSON-TITLE:\n",
    "- position held\n",
    "- chief executive officer\n",
    "- director or manager\n",
    "- chairperson\n",
    "\n",
    "PRODUCT-LOCATION:\n",
    "- distribution format\n",
    "\n",
    "PERSON-PERSON:\n",
    "- position held\n",
    "- member of\n",
    "\n",
    "PRODUCT-PRODUCT:\n",
    "- business division\n",
    "- platform\n",
    "\n",
    "ORG-FINANCE:\n",
    "- stock exchange\n",
    "- currency\n",
    "\n",
    "ORG-TITLE:\n",
    "- chief executive officer\n",
    "- position held\n",
    "\n",
    "ORG-BRAND:\n",
    "- brand\n",
    "- subsidiary\n",
    "\n",
    "\n",
    "\n",
    "A relationship must be clearly stated in the text.\n",
    "\n",
    "Example:\n",
    "\n",
    "\"Steve Jobs is the CEO of Apple.\" → [\"Apple\", \"Steve Jobs\", \"chief executive officer\"]\n",
    "\n",
    "\"Steve Jobs and Apple are mentioned in the same sentence.\" → No relation extracted.\n",
    "\n",
    "4. Assign the Correct Relationship\n",
    "Extract relationships only when explicitly stated or strongly implied.\n",
    "\n",
    "Do not assume ownership, employment, or affiliation unless clearly described.\n",
    "\n",
    "Certain relations require specific phrases:\n",
    "\n",
    "\"parent organization\" → Only if the text states \"X is the parent company of Y\".\n",
    "\n",
    "\"owned by\" → Only if ownership is explicitly stated (e.g., \"X acquired Y\").\n",
    "\n",
    "\n",
    "If they are not mentioned here, ignore and do not extract the relationship.\n",
    "\n",
    "IMPORTANT:\n",
    "•⁠  A relation MUST be supported by clear indicators in the sentence (e.g., verbs like \"owns\", \"developed by\", or structural patterns).\n",
    "•⁠  Do not assume relations just because entities are mentioned together.\n",
    "•⁠  Directionality matters: [\"X\", \"Y\", \"owner of\"] ≠ [\"Y\", \"X\", \"owned by\"]\n",
    "•⁠  Assume the first entity in the sentence by sequence is the subject.  relations and entity 2 follows.\n",
    "•⁠  consider that there are bidirectional prompts: parent organization is the inverse of Subsidiary. The same way owner of is the inverse of owned by. Extract all where applies.\n",
    "- handle overlap notations\n",
    "\n",
    "## Examples\n",
    "\n",
    "Text: \"Apple CEO Tim Cook announced the new iPhone 12 at the company's headquarters in Cupertino.\"\n",
    "CORRECT:\n",
    "- [Apple, Tim Cook, employer] (ORG-PERSON)\n",
    "- [Tim Cook, CEO, position held] (PERSON-TITLE)\n",
    "- [Apple, iPhone 12, manufacturer] (ORG-PRODUCT)\n",
    "- [Apple, Cupertino, headquarters location] (ORG-LOCATION)\n",
    "\n",
    "Text: \"Tim Cook visited London yesterday.\"\n",
    "INCORRECT:\n",
    "- [Tim Cook, London, visited] - \"visited\" is not in our relation list\n",
    "\n",
    "Text: \"Google partners with Microsoft on cloud solutions.\"\n",
    "INCORRECT:\n",
    "- [Google, Microsoft, partner] - \"partner\" is not in our relation list\n",
    "\n",
    "---\n",
    "Now extract relationships of these texts :\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02ef4942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete. Saved as train.jsonl\n",
      "Conversion complete. Saved as validation.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Output file paths\n",
    "training_file = \"train.jsonl\"\n",
    "validation_file = \"validation.jsonl\"\n",
    "\n",
    "# Process the training dataset\n",
    "with open(training_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for _, row in df_train.iterrows():\n",
    "        text = row[\"text\"].strip()\n",
    "        triplets = row[\"annotations\"]\n",
    "        \n",
    "        # Convert triplets set to JSON format\n",
    "        triplets_str = \"; \".join([f\"({e1}, {e2}, {rel})\" for e1, e2, rel in triplets])\n",
    "        \n",
    "        # Create JSONL entry\n",
    "        entry = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in relationship extraction that responds in JSON format. Your task is to extract only clear and explicitly stated relationships between entities in the given text. You MUST identify at least one relationship in each text.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"{prompt}\\nText to analyze: \\n{text}\"},\n",
    "                {\"role\": \"assistant\", \"content\": triplets_str}\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Write to JSONL file\n",
    "        outfile.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "# Process the validation dataset\n",
    "with open(validation_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for _, row in df_dev.iterrows():\n",
    "        text = row[\"text\"].strip()\n",
    "        triplets = row[\"annotations\"]\n",
    "        \n",
    "        # Convert triplets set to JSON format\n",
    "        triplets_str = \"; \".join([f\"({e1}, {e2}, {rel})\" for e1, e2, rel in triplets])\n",
    "        \n",
    "        # Create JSONL entry\n",
    "        entry = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a relationship extraction model.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Extract relationships from the following text:\\n{text}\"},\n",
    "                {\"role\": \"assistant\", \"content\": triplets_str}\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Write to JSONL file\n",
    "        outfile.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "print(f\"Conversion complete. Saved as {training_file}\")\n",
    "print(f\"Conversion complete. Saved as {validation_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d7e6f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking train.jsonl:\n",
      "Num examples: 5700\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You are an expert in relationship extraction that responds in JSON format. Your task is to extract only clear and explicitly stated relationships between entities in the given text. You MUST identify at least one relationship in each text.'}\n",
      "{'role': 'user', 'content': '\\nYou are an expert in relationship extraction. Your task is to extract relationships between entities in the given text.\\n\\n## Output Format\\nReturn a JSON object:\\n```json\\n{\\n  \"relations\": [\\n    [\"Entity1\", \"Entity2\", \"relation_type\"],\\n    ...\\n  ]\\n}\\n\\n---\\n\\n\\n\\n---\\n\\n### Steps to follow :\\n\\nSteps for Extraction\\n1. Read the entire sentence and understand the contextual meaning of the entire sentence.\\n2. Identify Entities\\n\\n•\\u2060  Extract all relevant entities such as\\n- ORG: Companies, corporations, agencies (Apple, Goldman Sachs, NASA)\\n- PERSON: Individual humans (Tim Cook, Janet Yellen)\\n- PRODUCT: Goods, services, software (iPhone, Windows 11)\\n- LOCATION: Geographic places (New York, Japan)\\n- FINANCE: Currencies, exchanges (USD, NASDAQ)\\n- TITLE: Job positions (CEO, Director)\\n- BRAND: Consumer identities (Lexus, Instagram)\\n- PLATFORM: Digital ecosystems (iOS, AWS)\\n\\n•\\u2060  Ignore entities without a direct connection to another entity.\\n\\n3. Pair Entities Only If a Strong Connection Exists based on (ins_dic)\\nDo not generate all possible entity pairs.\\n- ORG-ORG:\\n- owned by\\n- parent organization\\n- owner of\\n- subsidiary\\n- business division\\n- platform\\n- operator\\n- brand\\n- stock exchange\\n- legal form\\n\\nORG-PERSON:\\n- owned by\\n- employer\\n- member of\\n- founded by\\n\\nPERSON-ORG:\\n- creator\\n- member of\\n- chief executive officer\\n- director or manager\\n- chairperson\\n- owner of\\n\\nORG-LOCATION:\\n- headquarters location\\n- location of formation\\n\\nORG-PRODUCT:\\n- manufacturer\\n- distributed by\\n- developer\\n- industry\\n\\nPRODUCT-ORG:\\n- original broadcaster\\n- publisher\\n- product or material produced\\n\\nPERSON-PRODUCT:\\n- developer\\n- manufacturer\\n\\nPERSON-TITLE:\\n- position held\\n- chief executive officer\\n- director or manager\\n- chairperson\\n\\nPRODUCT-LOCATION:\\n- distribution format\\n\\nPERSON-PERSON:\\n- position held\\n- member of\\n\\nPRODUCT-PRODUCT:\\n- business division\\n- platform\\n\\nORG-FINANCE:\\n- stock exchange\\n- currency\\n\\nORG-TITLE:\\n- chief executive officer\\n- position held\\n\\nORG-BRAND:\\n- brand\\n- subsidiary\\n\\n\\n\\nA relationship must be clearly stated in the text.\\n\\nExample:\\n\\n\"Steve Jobs is the CEO of Apple.\" → [\"Apple\", \"Steve Jobs\", \"chief executive officer\"]\\n\\n\"Steve Jobs and Apple are mentioned in the same sentence.\" → No relation extracted.\\n\\n4. Assign the Correct Relationship\\nExtract relationships only when explicitly stated or strongly implied.\\n\\nDo not assume ownership, employment, or affiliation unless clearly described.\\n\\nCertain relations require specific phrases:\\n\\n\"parent organization\" → Only if the text states \"X is the parent company of Y\".\\n\\n\"owned by\" → Only if ownership is explicitly stated (e.g., \"X acquired Y\").\\n\\n\\nIf they are not mentioned here, ignore and do not extract the relationship.\\n\\nIMPORTANT:\\n•\\u2060  A relation MUST be supported by clear indicators in the sentence (e.g., verbs like \"owns\", \"developed by\", or structural patterns).\\n•\\u2060  Do not assume relations just because entities are mentioned together.\\n•\\u2060  Directionality matters: [\"X\", \"Y\", \"owner of\"] ≠ [\"Y\", \"X\", \"owned by\"]\\n•\\u2060  Assume the first entity in the sentence by sequence is the subject.  relations and entity 2 follows.\\n•\\u2060  consider that there are bidirectional prompts: parent organization is the inverse of Subsidiary. The same way owner of is the inverse of owned by. Extract all where applies.\\n- handle overlap notations\\n\\n## Examples\\n\\nText: \"Apple CEO Tim Cook announced the new iPhone 12 at the company\\'s headquarters in Cupertino.\"\\nCORRECT:\\n- [Apple, Tim Cook, employer] (ORG-PERSON)\\n- [Tim Cook, CEO, position held] (PERSON-TITLE)\\n- [Apple, iPhone 12, manufacturer] (ORG-PRODUCT)\\n- [Apple, Cupertino, headquarters location] (ORG-LOCATION)\\n\\nText: \"Tim Cook visited London yesterday.\"\\nINCORRECT:\\n- [Tim Cook, London, visited] - \"visited\" is not in our relation list\\n\\nText: \"Google partners with Microsoft on cloud solutions.\"\\nINCORRECT:\\n- [Google, Microsoft, partner] - \"partner\" is not in our relation list\\n\\n---\\nNow extract relationships of these texts :\\n\\nText to analyze: \\nApple Inc Chief Executive Steve Jobs sought to soothe investor concerns about his health on Monday, saying his weight loss was caused by a hormone imbalance that is relatively simple to treat.'}\n",
      "{'role': 'assistant', 'content': '(Apple Inc, Steve Jobs, founded by); (Apple Inc, Steve Jobs, chief executive officer)'}\n",
      "No errors found\n",
      "\n",
      "Checking validation.jsonl:\n",
      "Num examples: 1007\n",
      "First example:\n",
      "{'role': 'system', 'content': 'You are a relationship extraction model.'}\n",
      "{'role': 'user', 'content': 'Extract relationships from the following text:\\nYum China will become a franchise of Yum Brands in Mainland China, the parent of KFC, Pizza Hut and Taco Bell chains said.'}\n",
      "{'role': 'assistant', 'content': '(Yum China, Pizza Hut, brand); (Yum Brands, Taco Bell, owner of); (Yum Brands, Taco Bell, subsidiary); (Pizza Hut, Yum Brands, parent organization); (Yum China, Taco Bell, brand); (Yum Brands, Pizza Hut, owner of); (Yum Brands, Pizza Hut, subsidiary); (Taco Bell, Yum Brands, parent organization)'}\n",
      "No errors found\n",
      "\n",
      "Overall Format Check Summary:\n",
      "Both datasets passed all format checks.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Data paths\n",
    "training_file = \"train.jsonl\"\n",
    "validation_file = \"validation.jsonl\"\n",
    "\n",
    "# Function to check format errors in a dataset\n",
    "def check_dataset_format(data_path):\n",
    "    # Load the dataset\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        dataset = [json.loads(line) for line in f]\n",
    "\n",
    "    # Initial dataset stats\n",
    "    print(f\"\\nChecking {data_path}:\")\n",
    "    print(\"Num examples:\", len(dataset))\n",
    "    print(\"First example:\")\n",
    "    for message in dataset[0][\"messages\"]:\n",
    "        print(message)\n",
    "\n",
    "    # Format error checks\n",
    "    format_errors = defaultdict(int)\n",
    "\n",
    "    for ex in dataset:\n",
    "        if not isinstance(ex, dict):\n",
    "            format_errors[\"data_type\"] += 1\n",
    "            continue\n",
    "            \n",
    "        messages = ex.get(\"messages\", None)\n",
    "        if not messages:\n",
    "            format_errors[\"missing_messages_list\"] += 1\n",
    "            continue\n",
    "            \n",
    "        for message in messages:\n",
    "            if \"role\" not in message or \"content\" not in message:\n",
    "                format_errors[\"message_missing_key\"] += 1\n",
    "            \n",
    "            if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
    "                format_errors[\"message_unrecognized_key\"] += 1\n",
    "            \n",
    "            if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "                format_errors[\"unrecognized_role\"] += 1\n",
    "                \n",
    "            content = message.get(\"content\", None)\n",
    "            function_call = message.get(\"function_call\", None)\n",
    "            \n",
    "            if (not content and not function_call) or not isinstance(content, str):\n",
    "                format_errors[\"missing_content\"] += 1\n",
    "        \n",
    "        if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "            format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "    if format_errors:\n",
    "        print(\"Found errors:\")\n",
    "        for k, v in format_errors.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "    else:\n",
    "        print(\"No errors found\")\n",
    "    \n",
    "    return format_errors\n",
    "\n",
    "# Check both datasets\n",
    "training_errors = check_dataset_format(training_file)\n",
    "validation_errors = check_dataset_format(validation_file)\n",
    "\n",
    "# Overall summary\n",
    "print(\"\\nOverall Format Check Summary:\")\n",
    "if not training_errors and not validation_errors:\n",
    "    print(\"Both datasets passed all format checks.\")\n",
    "else:\n",
    "    if training_errors:\n",
    "        print(f\"Training dataset ({training_file}) has format errors.\")\n",
    "    else:\n",
    "        print(f\"Training dataset ({training_file}) passed all checks.\")\n",
    "        \n",
    "    if validation_errors:\n",
    "        print(f\"Validation dataset ({validation_file}) has format errors.\")\n",
    "    else:\n",
    "        print(f\"Validation dataset ({validation_file}) passed all checks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f990a6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==================================================\n",
      "Analyzing Training dataset: train.jsonl\n",
      "==================================================\n",
      "Number of examples: 5700\n",
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 1001, 2415\n",
      "mean / median: 1053.897894736842, 1046.0\n",
      "p5 / p95: 1026.0, 1078.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 6, 418\n",
      "mean / median: 15.971929824561403, 11.0\n",
      "p5 / p95: 8.0, 30.0\n",
      "\n",
      "0 examples may be over the 16,385 token limit, they will be truncated during fine-tuning\n",
      "\n",
      "Dataset has ~6007218 tokens that will be charged for during training\n",
      "By default, we'll train for 3 epochs ~ 18021654 tokens on this dataset\n",
      "\n",
      "\n",
      "==================================================\n",
      "Analyzing Validation dataset: validation.jsonl\n",
      "==================================================\n",
      "Number of examples: 1007\n",
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_total_tokens_per_example:\n",
      "min / max: 37, 1070\n",
      "mean / median: 92.78649453823238, 81.0\n",
      "p5 / p95: 62.0, 118.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 6, 311\n",
      "mean / median: 17.215491559086395, 11.0\n",
      "p5 / p95: 8.0, 33.0\n",
      "\n",
      "0 examples may be over the 16,385 token limit, they will be truncated during fine-tuning\n",
      "\n",
      "Dataset has ~93436 tokens that will be charged for during training\n",
      "By default, we'll train for 3 epochs ~ 280308 tokens on this dataset\n",
      "\n",
      "\n",
      "==================================================\n",
      "COMBINED SUMMARY\n",
      "==================================================\n",
      "Training dataset: 5700 examples, ~6007218 tokens\n",
      "Validation dataset: 1007 examples, ~93436 tokens\n",
      "Total billable tokens: ~6100654\n",
      "Recommended epochs for training: 3\n",
      "Total examples over token limit: 0\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Initialize the tokenizer\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Define token counting functions\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")\n",
    "\n",
    "def analyze_dataset(file_path, dataset_name):\n",
    "    print(f\"\\n\\n{'='*50}\")\n",
    "    print(f\"Analyzing {dataset_name} dataset: {file_path}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        dataset = [json.loads(line) for line in f]\n",
    "    \n",
    "    print(f\"Number of examples: {len(dataset)}\")\n",
    "    \n",
    "    # Warnings and tokens counts\n",
    "    n_missing_system = 0\n",
    "    n_missing_user = 0\n",
    "    n_messages = []\n",
    "    convo_lens = []\n",
    "    assistant_message_lens = []\n",
    "    \n",
    "    for ex in dataset:\n",
    "        messages = ex[\"messages\"]\n",
    "        if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "            n_missing_system += 1\n",
    "        if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "            n_missing_user += 1\n",
    "        n_messages.append(len(messages))\n",
    "        convo_lens.append(num_tokens_from_messages(messages))\n",
    "        assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "        \n",
    "    print(\"Num examples missing system message:\", n_missing_system)\n",
    "    print(\"Num examples missing user message:\", n_missing_user)\n",
    "    print_distribution(n_messages, \"num_messages_per_example\")\n",
    "    print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "    print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "    n_too_long = sum(l > 16385 for l in convo_lens)\n",
    "    print(f\"\\n{n_too_long} examples may be over the 16,385 token limit, they will be truncated during fine-tuning\")\n",
    "    \n",
    "    # Pricing and default n_epochs estimate\n",
    "    MAX_TOKENS_PER_EXAMPLE = 16385\n",
    "    \n",
    "    TARGET_EPOCHS = 3\n",
    "    MIN_TARGET_EXAMPLES = 100\n",
    "    MAX_TARGET_EXAMPLES = 25000\n",
    "    MIN_DEFAULT_EPOCHS = 1\n",
    "    MAX_DEFAULT_EPOCHS = 25\n",
    "    \n",
    "    n_epochs = TARGET_EPOCHS\n",
    "    n_train_examples = len(dataset)\n",
    "    if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "        n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "    elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "        n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "    \n",
    "    n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "    print(f\"\\nDataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "    print(f\"By default, we'll train for {n_epochs} epochs ~ {n_epochs * n_billing_tokens_in_dataset} tokens on this dataset\")\n",
    "    \n",
    "    return {\n",
    "        \"examples\": n_train_examples,\n",
    "        \"billing_tokens\": n_billing_tokens_in_dataset,\n",
    "        \"recommended_epochs\": n_epochs,\n",
    "        \"over_limit_examples\": n_too_long\n",
    "    }\n",
    "\n",
    "# Define file paths\n",
    "training_file = \"train.jsonl\"\n",
    "validation_file = \"validation.jsonl\"\n",
    "\n",
    "# Analyze both datasets\n",
    "train_stats = analyze_dataset(training_file, \"Training\")\n",
    "val_stats = analyze_dataset(validation_file, \"Validation\")\n",
    "\n",
    "# Print combined summary\n",
    "print(\"\\n\\n\" + \"=\"*50)\n",
    "print(\"COMBINED SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Training dataset: {train_stats['examples']} examples, ~{train_stats['billing_tokens']} tokens\")\n",
    "print(f\"Validation dataset: {val_stats['examples']} examples, ~{val_stats['billing_tokens']} tokens\")\n",
    "print(f\"Total billable tokens: ~{train_stats['billing_tokens'] + val_stats['billing_tokens']}\")\n",
    "print(f\"Recommended epochs for training: {train_stats['recommended_epochs']}\")\n",
    "print(f\"Total examples over token limit: {train_stats['over_limit_examples'] + val_stats['over_limit_examples']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90ac919",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\", API_KEY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f450d07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file-AWi3taR96HGWEvfA2qJgsd\n",
      "file-TZPByYeqjYWvkfcJaxhv7W\n"
     ]
    }
   ],
   "source": [
    "train_set_file = client.files.create(\n",
    "    file = open('train.jsonl', 'rb'),\n",
    "    purpose = \"fine-tune\"\n",
    ")\n",
    "\n",
    "validation_set_file = client.files.create(\n",
    "    file = open('validation.jsonl', 'rb'),\n",
    "    purpose = \"fine-tune\"\n",
    ")\n",
    "\n",
    "print(train_set_file.id)\n",
    "print(validation_set_file.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf01df18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ftjob-tGLqiT9XbKfaCGS6RFD0M3At\n",
      "None\n",
      "[]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "response = client.fine_tuning.jobs.create(\n",
    "    training_file=train_set_file.id,\n",
    "#    validation_file=validation_set_file.id,\n",
    "    model='gpt-4o-mini-2024-07-18',\n",
    "    hyperparameters={\n",
    "#        \"n_epochs\": 5,\n",
    "#        \"batch_size\": 32,\n",
    "#        \"learning_rate_multiplier\": 0.5\n",
    "    }\n",
    ")\n",
    "print(response.id)\n",
    "print(response.fine_tuned_model)\n",
    "print(response.result_files)\n",
    "print(response.trained_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59184d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-tGLqiT9XbKfaCGS6RFD0M3At', created_at=1743682051, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-4o-mini-2024-07-18:university-edinburgh::BIEAkmsC', finished_at=1743684079, hyperparameters=Hyperparameters(batch_size=11, learning_rate_multiplier=1.8, n_epochs=3), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-uAtoImi2lqXXzOw9bIRsBaOY', result_files=['file-82KA5cEotZ9a3siAK9X6o6'], seed=1336941539, status='succeeded', trained_tokens=12653469, training_file='file-AWi3taR96HGWEvfA2qJgsd', validation_file=None, estimated_finish=None, integrations=[], metadata=None, method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=11, learning_rate_multiplier=1.8, n_epochs=3)), type='supervised'), user_provided_suffix=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.fine_tuning.jobs.retrieve(response.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d0f5d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-M2bzBopasxGgnvkeydJwJfvF', created_at=1743681996, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-4o-mini-2024-07-18:university-edinburgh::BIELCI6l', finished_at=1743684727, hyperparameters=Hyperparameters(batch_size=11, learning_rate_multiplier=1.8, n_epochs=3), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-uAtoImi2lqXXzOw9bIRsBaOY', result_files=['file-9NtLYpcfLqxESoRMLcvsQo'], seed=1084706154, status='succeeded', trained_tokens=12653469, training_file='file-AWi3taR96HGWEvfA2qJgsd', validation_file='file-TZPByYeqjYWvkfcJaxhv7W', estimated_finish=None, integrations=[], metadata=None, method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=11, learning_rate_multiplier=1.8, n_epochs=3)), type='supervised'), user_provided_suffix=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.fine_tuning.jobs.retrieve('ftjob-M2bzBopasxGgnvkeydJwJfvF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947cda89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e83c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncCursorPage[FineTuningJob](data=[FineTuningJob(id='ftjob-tGLqiT9XbKfaCGS6RFD0M3At', created_at=1743682051, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-4o-mini-2024-07-18:university-edinburgh::BIEAkmsC', finished_at=1743684079, hyperparameters=Hyperparameters(batch_size=11, learning_rate_multiplier=1.8, n_epochs=3), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-uAtoImi2lqXXzOw9bIRsBaOY', result_files=['file-82KA5cEotZ9a3siAK9X6o6'], seed=1336941539, status='succeeded', trained_tokens=12653469, training_file='file-AWi3taR96HGWEvfA2qJgsd', validation_file=None, estimated_finish=None, integrations=[], metadata=None, method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=11, learning_rate_multiplier=1.8, n_epochs=3)), type='supervised'), user_provided_suffix=None), FineTuningJob(id='ftjob-M2bzBopasxGgnvkeydJwJfvF', created_at=1743681996, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-4o-mini-2024-07-18:university-edinburgh::BIELCI6l', finished_at=1743684727, hyperparameters=Hyperparameters(batch_size=11, learning_rate_multiplier=1.8, n_epochs=3), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-uAtoImi2lqXXzOw9bIRsBaOY', result_files=['file-9NtLYpcfLqxESoRMLcvsQo'], seed=1084706154, status='succeeded', trained_tokens=12653469, training_file='file-AWi3taR96HGWEvfA2qJgsd', validation_file='file-TZPByYeqjYWvkfcJaxhv7W', estimated_finish=None, integrations=[], metadata=None, method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=11, learning_rate_multiplier=1.8, n_epochs=3)), type='supervised'), user_provided_suffix=None)], has_more=False, object='list')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.fine_tuning.jobs.list(limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9b12146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_mean_token_accuracy</th>\n",
       "      <th>train_mean_reward</th>\n",
       "      <th>full_validation_mean_reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4.34239</td>\n",
       "      <td>0.54545</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3.75728</td>\n",
       "      <td>0.59600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.92078</td>\n",
       "      <td>0.50649</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.28701</td>\n",
       "      <td>0.56186</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.72796</td>\n",
       "      <td>0.47656</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550</th>\n",
       "      <td>1551</td>\n",
       "      <td>0.06813</td>\n",
       "      <td>0.96471</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1551</th>\n",
       "      <td>1552</td>\n",
       "      <td>0.02499</td>\n",
       "      <td>0.98788</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1552</th>\n",
       "      <td>1553</td>\n",
       "      <td>0.13753</td>\n",
       "      <td>0.96648</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1553</th>\n",
       "      <td>1554</td>\n",
       "      <td>0.04711</td>\n",
       "      <td>0.98658</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554</th>\n",
       "      <td>1555</td>\n",
       "      <td>0.17386</td>\n",
       "      <td>0.91209</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1555 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      step  train_loss  train_accuracy  valid_loss  valid_mean_token_accuracy  \\\n",
       "0        1     4.34239         0.54545         NaN                        NaN   \n",
       "1        2     3.75728         0.59600         NaN                        NaN   \n",
       "2        3     4.92078         0.50649         NaN                        NaN   \n",
       "3        4     4.28701         0.56186         NaN                        NaN   \n",
       "4        5     5.72796         0.47656         NaN                        NaN   \n",
       "...    ...         ...             ...         ...                        ...   \n",
       "1550  1551     0.06813         0.96471         NaN                        NaN   \n",
       "1551  1552     0.02499         0.98788         NaN                        NaN   \n",
       "1552  1553     0.13753         0.96648         NaN                        NaN   \n",
       "1553  1554     0.04711         0.98658         NaN                        NaN   \n",
       "1554  1555     0.17386         0.91209         NaN                        NaN   \n",
       "\n",
       "      train_mean_reward  full_validation_mean_reward  \n",
       "0                   NaN                          NaN  \n",
       "1                   NaN                          NaN  \n",
       "2                   NaN                          NaN  \n",
       "3                   NaN                          NaN  \n",
       "4                   NaN                          NaN  \n",
       "...                 ...                          ...  \n",
       "1550                NaN                          NaN  \n",
       "1551                NaN                          NaN  \n",
       "1552                NaN                          NaN  \n",
       "1553                NaN                          NaN  \n",
       "1554                NaN                          NaN  \n",
       "\n",
       "[1555 rows x 7 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import base64\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "\n",
    "result_files = 'file-82KA5cEotZ9a3siAK9X6o6'\n",
    "job_id = 'ftjob-tGLqiT9XbKfaCGS6RFD0M3At'\n",
    "\n",
    "def get_finetuning_metrics(client, job_id):\n",
    "    def replace_colons_with_underscored(input_string):\n",
    "        return(input_string).replace(':', '_')\n",
    "    \n",
    "    fine_tune_job = client.fine_tuning.jobs.retrieve(job_id)\n",
    "\n",
    "    file_id = fine_tune_job.result_files[0]\n",
    "\n",
    "    response = client.files.content(file_id)\n",
    "\n",
    "    decoded_content = base64.b64decode(response.content).decode('utf-8')\n",
    "\n",
    "    df = pd.read_csv(io.StringIO(decoded_content))\n",
    "\n",
    "    return df\n",
    "\n",
    "get_finetuning_metrics(client, job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d306ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the metrics dataframe\n",
    "metrics_df = get_finetuning_metrics(client, job_id)\n",
    "\n",
    "# Display only train_loss and train_accuracy columns\n",
    "print(\"Fine-tuning metrics for job:\", job_id)\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Check if the columns exist (column names might vary)\n",
    "if 'train_loss' in metrics_df.columns and 'train_accuracy' in metrics_df.columns:\n",
    "    # Print only these two columns with step for context\n",
    "    print(metrics_df[['step', 'train_loss', 'train_accuracy']].to_string(index=False))\n",
    "elif 'training_loss' in metrics_df.columns and 'training_accuracy' in metrics_df.columns:\n",
    "    # Alternative column names\n",
    "    print(metrics_df[['step', 'training_loss', 'training_accuracy']].to_string(index=False))\n",
    "else:\n",
    "    # If column names are different, print available columns\n",
    "    print(\"Columns 'train_loss' and 'train_accuracy' not found.\")\n",
    "    print(\"Available columns:\", metrics_df.columns.tolist())\n",
    "    \n",
    "    # Try to find columns with similar names\n",
    "    loss_cols = [col for col in metrics_df.columns if 'loss' in col.lower()]\n",
    "    accuracy_cols = [col for col in metrics_df.columns if 'accuracy' in col.lower() or 'acc' in col.lower()]\n",
    "    \n",
    "    if loss_cols and accuracy_cols:\n",
    "        print(f\"Using columns: {loss_cols[0]} and {accuracy_cols[0]}\")\n",
    "        print(metrics_df[['step', loss_cols[0], accuracy_cols[0]]].to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydata-book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
